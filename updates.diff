diff --git a/Cargo.lock b/Cargo.lock
index ac988a1de..fc6e5123b 100644
--- a/Cargo.lock
+++ b/Cargo.lock
@@ -2432,7 +2432,7 @@ dependencies = [
 [[package]]
 name = "librocksdb_sys"
 version = "0.1.0"
-source = "git+https://github.com/tikv/rust-rocksdb.git?branch=tikv-5.2#23bd00d50c79b40b6a32c11446c86f0714fa7844"
+source = "git+https://github.com/shawgerj/rust-rocksdb.git?branch=writefail"
 dependencies = [
  "bindgen",
  "bzip2-sys",
@@ -2451,7 +2451,7 @@ dependencies = [
 [[package]]
 name = "libtitan_sys"
 version = "0.0.1"
-source = "git+https://github.com/tikv/rust-rocksdb.git?branch=tikv-5.2#23bd00d50c79b40b6a32c11446c86f0714fa7844"
+source = "git+https://github.com/shawgerj/rust-rocksdb.git?branch=writefail"
 dependencies = [
  "bzip2-sys",
  "cc",
@@ -3121,31 +3121,12 @@ dependencies = [
  "regex",
 ]
 
-[[package]]
-name = "paste"
-version = "0.1.18"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "45ca20c77d80be666aef2b45486da86238fabe33e38306bd3118fe4af33fa880"
-dependencies = [
- "paste-impl",
- "proc-macro-hack",
-]
-
 [[package]]
 name = "paste"
 version = "1.0.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "c5d65c4d95931acda4498f675e332fcbdc9a06705cd07086c510e9b6009cd1c1"
 
-[[package]]
-name = "paste-impl"
-version = "0.1.18"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d95a7db200b97ef370c8e6de0088252f7e0dfff7d047a28528e47456c0fc98b6"
-dependencies = [
- "proc-macro-hack",
-]
-
 [[package]]
 name = "pd_client"
 version = "0.1.0"
@@ -4196,7 +4177,7 @@ dependencies = [
 [[package]]
 name = "rocksdb"
 version = "0.3.0"
-source = "git+https://github.com/tikv/rust-rocksdb.git?branch=tikv-5.2#23bd00d50c79b40b6a32c11446c86f0714fa7844"
+source = "git+https://github.com/shawgerj/rust-rocksdb.git?branch=writefail"
 dependencies = [
  "libc 0.2.106",
  "librocksdb_sys",
@@ -5256,7 +5237,7 @@ dependencies = [
  "more-asserts",
  "online_config",
  "panic_hook",
- "paste 1.0.4",
+ "paste",
  "pd_client",
  "perfcnt",
  "profiler",
@@ -5553,7 +5534,7 @@ dependencies = [
  "openssl",
  "panic_hook",
  "parking_lot",
- "paste 1.0.4",
+ "paste",
  "pd_client",
  "pin-project",
  "pnet_datalink",
@@ -5664,20 +5645,20 @@ dependencies = [
 
 [[package]]
 name = "tikv-jemalloc-ctl"
-version = "0.4.1"
+version = "0.4.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f28c80e4338857639f443169a601fafe49866aed8d7a8d565c2f5bfb1a021adf"
+checksum = "eb833c46ecbf8b6daeccb347cefcabf9c1beb5c9b0f853e1cec45632d9963e69"
 dependencies = [
  "libc 0.2.106",
- "paste 0.1.18",
+ "paste",
  "tikv-jemalloc-sys",
 ]
 
 [[package]]
 name = "tikv-jemalloc-sys"
-version = "0.4.1+5.2.1-patched"
+version = "0.4.3+5.2.1-patched.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8a26331b05179d4cb505c8d6814a7e18d298972f0a551b0e3cefccff927f86d3"
+checksum = "a1792ccb507d955b46af42c123ea8863668fae24d03721e40cad6a41773dbb49"
 dependencies = [
  "cc",
  "fs_extra",
@@ -5686,9 +5667,9 @@ dependencies = [
 
 [[package]]
 name = "tikv-jemallocator"
-version = "0.4.1"
+version = "0.4.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3c14a5a604eb8715bc5785018a37d00739b180bcf609916ddf4393d33d49ccdf"
+checksum = "a5b7bcecfafe4998587d636f9ae9d55eb9d0499877b88757767c346875067098"
 dependencies = [
  "libc 0.2.106",
  "tikv-jemalloc-sys",
@@ -6444,12 +6425,10 @@ dependencies = [
 
 [[package]]
 name = "zstd-sys"
-version = "1.4.19+zstd.1.4.8"
+version = "1.6.3+zstd.1.5.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ec24a9273d24437afb8e71b16f3d9a5d569193cccdb7896213b59f552f387674"
+checksum = "fc49afa5c8d634e75761feda8c592051e7eeb4683ba827211eb0d731d3402ea8"
 dependencies = [
  "cc",
- "glob",
- "itertools 0.9.0",
  "libc 0.2.106",
 ]
diff --git a/Cargo.toml b/Cargo.toml
index 7f0d16258..3fa13bc65 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -329,7 +329,7 @@ default-members = ["cmd/tikv-server", "cmd/tikv-ctl"]
 
 [profile.dev]
 opt-level = 0
-debug = 1 # required for line numbers in tests, see tikv #5049
+debug = true # required for line numbers in tests, see tikv #5049
 codegen-units = 4
 lto = false
 incremental = true
@@ -355,7 +355,7 @@ codegen-units = 4
 
 [profile.test]
 opt-level = 0
-debug = 1 # enable line numbers by default for easy test debugging
+debug = true # enable line numbers by default for easy test debugging
 codegen-units = 16
 lto = false
 incremental = true
diff --git a/cmd/tikv-server/src/main.rs b/cmd/tikv-server/src/main.rs
index 8950ba38a..9dd62eb81 100644
--- a/cmd/tikv-server/src/main.rs
+++ b/cmd/tikv-server/src/main.rs
@@ -18,6 +18,13 @@ fn main() {
         .author(crate_authors!())
         .version(version_info.as_ref())
         .long_version(version_info.as_ref())
+        .arg(
+             Arg::with_name("fail-on-write")
+                 .long("fail-on-write")
+                 .required(false)
+                 .takes_value(false)
+                 .help("Configure RocksDB to not persist data on exit"),
+        )
         .arg(
             Arg::with_name("config")
                 .short("C")
diff --git a/components/engine_panic/src/engine.rs b/components/engine_panic/src/engine.rs
index fd76c5acf..5b7288fa8 100644
--- a/components/engine_panic/src/engine.rs
+++ b/components/engine_panic/src/engine.rs
@@ -20,6 +20,9 @@ impl KvEngine for PanicEngine {
     fn sync(&self) -> Result<()> {
         panic!()
     }
+    fn flush_all(&self) -> Result<()> {
+        panic!()
+    }
     fn bad_downcast<T: 'static>(&self) -> &T {
         panic!()
     }
diff --git a/components/engine_panic/src/misc.rs b/components/engine_panic/src/misc.rs
index 1abb062f8..4ffcd6c07 100644
--- a/components/engine_panic/src/misc.rs
+++ b/components/engine_panic/src/misc.rs
@@ -12,6 +12,10 @@ impl MiscExt for PanicEngine {
         panic!()
     }
 
+    fn flush_wal(&self, sync: bool) -> Result<()> {
+        panic!()
+    }
+
     fn delete_ranges_cf(&self, cf: &str, strategy: DeleteStrategy, ranges: &[Range]) -> Result<()> {
         panic!()
     }
diff --git a/components/engine_panic/src/raft_engine.rs b/components/engine_panic/src/raft_engine.rs
index 54e394a22..5ea6f656a 100644
--- a/components/engine_panic/src/raft_engine.rs
+++ b/components/engine_panic/src/raft_engine.rs
@@ -3,7 +3,7 @@
 use crate::engine::PanicEngine;
 use crate::write_batch::PanicWriteBatch;
 use engine_traits::{Error, RaftEngine, RaftEngineReadOnly, RaftLogBatch, Result};
-use kvproto::raft_serverpb::RaftLocalState;
+use kvproto::raft_serverpb::{RaftLocalState, RaftApplyState, RegionLocalState};
 use raft::eraftpb::Entry;
 
 impl RaftEngineReadOnly for PanicEngine {
@@ -74,6 +74,22 @@ impl RaftEngine for PanicEngine {
         panic!()
     }
 
+    fn put_raft_apply_state(&self, id: u64, state: &RaftApplyState) -> Result<()> {
+        panic!()
+    }
+
+    fn put_raft_region_state(&self, id: u64, state: &RegionLocalState) -> Result<()> {
+        panic!()
+    }
+
+    fn get_raft_apply_state(&self, id: u64) -> Result<Option<RaftApplyState>> {
+        panic!()
+    }
+
+    fn get_raft_region_state(&self, id: u64) -> Result<Option<RegionLocalState>> {
+        panic!()
+    }
+
     fn gc(&self, raft_group_id: u64, mut from: u64, to: u64) -> Result<usize> {
         panic!()
     }
@@ -116,6 +132,14 @@ impl RaftLogBatch for PanicWriteBatch {
         panic!()
     }
 
+    fn put_raft_apply_state(&mut self, id: u64, state: &RaftApplyState) -> Result<()> {
+        panic!()
+    }
+
+    fn put_raft_region_state(&mut self, id: u64, state: &RegionLocalState) -> Result<()> {
+        panic!()
+    }
+
     fn persist_size(&self) -> usize {
         panic!()
     }
diff --git a/components/engine_rocks/Cargo.toml b/components/engine_rocks/Cargo.toml
index 5c2e9c065..3f351502a 100644
--- a/components/engine_rocks/Cargo.toml
+++ b/components/engine_rocks/Cargo.toml
@@ -71,10 +71,10 @@ protobuf = "2"
 fail = "0.4"
 
 [dependencies.rocksdb]
-git = "https://github.com/tikv/rust-rocksdb.git"
+git = "https://github.com/shawgerj/rust-rocksdb.git"
 package = "rocksdb"
 features = ["encryption", "static_libcpp"]
-branch = "tikv-5.2"
+branch = "writefail"
 
 [dev-dependencies]
 rand = "0.8"
diff --git a/components/engine_rocks/src/engine.rs b/components/engine_rocks/src/engine.rs
index 2ec323eef..211664593 100644
--- a/components/engine_rocks/src/engine.rs
+++ b/components/engine_rocks/src/engine.rs
@@ -4,11 +4,12 @@ use std::any::Any;
 use std::fs;
 use std::path::Path;
 use std::sync::Arc;
+use std::convert::TryInto;
 
 use engine_traits::{
-    Error, IterOptions, Iterable, KvEngine, Peekable, ReadOptions, Result, SyncMutable,
+    Error, IterOptions, Iterable, KvEngine, Peekable, ReadOptions, Result, SyncMutable, ALL_CFS
 };
-use rocksdb::{DBIterator, Writable, DB};
+use rocksdb::{DBIterator, Writable, DB, CFHandle};
 
 use crate::db_vector::RocksDBVector;
 use crate::options::RocksReadOptions;
@@ -76,6 +77,14 @@ impl KvEngine for RocksEngine {
         self.db.sync_wal().map_err(Error::Engine)
     }
 
+    fn flush_all(&self) -> Result<()> {
+        let cfs: Vec<&CFHandle> = ALL_CFS.iter().map(|cf| get_cf_handle(self.as_inner(), cf).unwrap()).collect();
+
+        let cfs_arr: &[&CFHandle; 4] = cfs[..].try_into().unwrap();
+        
+        self.db.flush_cfs(cfs_arr, false).map_err(Error::Engine)
+    }
+
     fn flush_metrics(&self, instance: &str) {
         for t in ENGINE_TICKER_TYPES {
             let v = self.db.get_and_reset_statistics_ticker_count(*t);
diff --git a/components/engine_rocks/src/file_system.rs b/components/engine_rocks/src/file_system.rs
index 91778273e..f3c0ab47f 100644
--- a/components/engine_rocks/src/file_system.rs
+++ b/components/engine_rocks/src/file_system.rs
@@ -19,6 +19,10 @@ pub(crate) fn get_env(
     )?))
 }
 
+pub(crate) fn get_fault_injection_env() -> Result<Arc<Env>, String> {
+    Ok(Arc::new(Env::new_fault_injection()))
+}
+
 pub struct WrappedFileSystemInspector<T: FileSystemInspector> {
     inspector: T,
 }
diff --git a/components/engine_rocks/src/lib.rs b/components/engine_rocks/src/lib.rs
index 349a9592f..8883c57d2 100644
--- a/components/engine_rocks/src/lib.rs
+++ b/components/engine_rocks/src/lib.rs
@@ -116,3 +116,7 @@ pub fn get_env(
     let env = encryption::get_env(None /*base_env*/, key_manager)?;
     file_system::get_env(Some(env), limiter)
 }
+
+pub fn get_fault_injection_env() -> std::result::Result<std::sync::Arc<raw::Env>, String> {
+    file_system::get_fault_injection_env()
+}
diff --git a/components/engine_rocks/src/misc.rs b/components/engine_rocks/src/misc.rs
index 4c49bb046..fc7daabcf 100644
--- a/components/engine_rocks/src/misc.rs
+++ b/components/engine_rocks/src/misc.rs
@@ -136,6 +136,10 @@ impl MiscExt for RocksEngine {
         Ok(self.as_inner().flush_cf(handle, sync)?)
     }
 
+    fn flush_wal(&self, sync: bool) -> Result<()> {
+        Ok(self.as_inner().flush_wal(sync)?)
+    }
+
     fn delete_ranges_cf(&self, cf: &str, strategy: DeleteStrategy, ranges: &[Range]) -> Result<()> {
         if ranges.is_empty() {
             return Ok(());
diff --git a/components/engine_rocks/src/options.rs b/components/engine_rocks/src/options.rs
index 39ca98d81..d9e0b387b 100644
--- a/components/engine_rocks/src/options.rs
+++ b/components/engine_rocks/src/options.rs
@@ -39,6 +39,7 @@ impl From<engine_traits::WriteOptions> for RocksWriteOptions {
     fn from(opts: engine_traits::WriteOptions) -> Self {
         let mut r = RawWriteOptions::default();
         r.set_sync(opts.sync());
+        r.disable_wal(opts.disable_wal());
         r.set_no_slowdown(opts.no_slowdown());
         RocksWriteOptions(r)
     }
diff --git a/components/engine_rocks/src/raft_engine.rs b/components/engine_rocks/src/raft_engine.rs
index df64211f0..eb747a42a 100644
--- a/components/engine_rocks/src/raft_engine.rs
+++ b/components/engine_rocks/src/raft_engine.rs
@@ -6,9 +6,9 @@ use crate::{util, RocksEngine, RocksWriteBatch};
 use engine_traits::{
     Error, Iterable, KvEngine, MiscExt, Mutable, Peekable, RaftEngine, RaftEngineReadOnly,
     RaftLogBatch, RaftLogGCTask, Result, SyncMutable, WriteBatch, WriteBatchExt, WriteOptions,
-    CF_DEFAULT,
+    CF_DEFAULT, CF_RAFT,
 };
-use kvproto::raft_serverpb::RaftLocalState;
+use kvproto::raft_serverpb::{RaftLocalState, RaftApplyState, RegionLocalState};
 use protobuf::Message;
 use raft::eraftpb::Entry;
 use tikv_util::{box_err, box_try};
@@ -153,7 +153,7 @@ impl RocksEngine {
 // every engine.
 impl RaftEngine for RocksEngine {
     type LogBatch = RocksWriteBatch;
-
+    
     fn log_batch(&self, capacity: usize) -> Self::LogBatch {
         RocksWriteBatch::with_capacity(self.as_inner().clone(), capacity)
     }
@@ -209,7 +209,7 @@ impl RaftEngine for RocksEngine {
             } else {
                 return Ok(());
             }
-        }
+         }
         if first_index <= state.last_index {
             for index in first_index..=state.last_index {
                 let key = keys::raft_log_key(raft_group_id, index);
@@ -230,6 +230,24 @@ impl RaftEngine for RocksEngine {
         self.put_msg(&keys::raft_state_key(raft_group_id), state)
     }
 
+    fn put_raft_apply_state(&self, id: u64, state: &RaftApplyState) -> Result<()> {
+        self.put_msg_cf(CF_RAFT, &keys::apply_state_key(id), state)
+    }
+
+    fn put_raft_region_state(&self, id: u64, state: &RegionLocalState) -> Result<()> {
+        self.put_msg_cf(CF_RAFT, &keys::region_state_key(id), state)
+    }
+
+    fn get_raft_apply_state(&self, id: u64) -> Result<Option<RaftApplyState>> {
+        let key = keys::apply_state_key(id);
+        self.get_msg_cf(CF_RAFT, &key)
+    }
+
+    fn get_raft_region_state(&self, id: u64) -> Result<Option<RegionLocalState>> {
+        let key = keys::region_state_key(id);
+        self.get_msg_cf(CF_RAFT, &key)
+    }
+
     fn batch_gc(&self, groups: Vec<RaftLogGCTask>) -> Result<usize> {
         let mut total = 0;
         let mut raft_wb = self.write_batch_with_cap(4 * 1024);
@@ -301,6 +319,14 @@ impl RaftLogBatch for RocksWriteBatch {
         self.put_msg(&keys::raft_state_key(raft_group_id), state)
     }
 
+    fn put_raft_apply_state(&mut self, id: u64, state: &RaftApplyState) -> Result<()> {
+        self.put_msg_cf(CF_RAFT, &keys::apply_state_key(id), state)
+    }
+
+    fn put_raft_region_state(&mut self, id: u64, state: &RegionLocalState) -> Result<()> {
+        self.put_msg_cf(CF_RAFT, &keys::region_state_key(id), state)
+    }
+
     fn persist_size(&self) -> usize {
         self.data_size()
     }
diff --git a/components/engine_traits/src/engine.rs b/components/engine_traits/src/engine.rs
index 13cdc5540..f0b92702d 100644
--- a/components/engine_traits/src/engine.rs
+++ b/components/engine_traits/src/engine.rs
@@ -42,6 +42,8 @@ pub trait KvEngine:
     /// Syncs any writes to disk
     fn sync(&self) -> Result<()>;
 
+    fn flush_all(&self) -> Result<()>;
+    
     /// Flush metrics to prometheus
     ///
     /// `instance` is the label of the metric to flush.
diff --git a/components/engine_traits/src/misc.rs b/components/engine_traits/src/misc.rs
index 1ae13f04c..5a2ded140 100644
--- a/components/engine_traits/src/misc.rs
+++ b/components/engine_traits/src/misc.rs
@@ -31,6 +31,8 @@ pub trait MiscExt: CFNamesExt + FlowControlFactorsExt {
 
     fn flush_cf(&self, cf: &str, sync: bool) -> Result<()>;
 
+    fn flush_wal(&self, sync: bool) -> Result<()>;
+
     fn delete_all_in_range(&self, strategy: DeleteStrategy, ranges: &[Range]) -> Result<()> {
         for cf in self.cf_names() {
             self.delete_ranges_cf(cf, strategy.clone(), ranges)?;
diff --git a/components/engine_traits/src/options.rs b/components/engine_traits/src/options.rs
index 19676d6a3..8a8b47e4f 100644
--- a/components/engine_traits/src/options.rs
+++ b/components/engine_traits/src/options.rs
@@ -33,6 +33,7 @@ impl Default for ReadOptions {
 pub struct WriteOptions {
     sync: bool,
     no_slowdown: bool,
+    disable_wal: bool,
 }
 
 impl WriteOptions {
@@ -40,6 +41,7 @@ impl WriteOptions {
         WriteOptions {
             sync: false,
             no_slowdown: false,
+            disable_wal: false,
         }
     }
 
@@ -51,6 +53,14 @@ impl WriteOptions {
         self.sync
     }
 
+    pub fn set_disable_wal(&mut self, disable: bool) {
+     	self.disable_wal = disable;
+    }
+
+    pub fn disable_wal(&self) -> bool {
+     	self.disable_wal
+    }
+
     pub fn set_no_slowdown(&mut self, no_slowdown: bool) {
         self.no_slowdown = no_slowdown;
     }
@@ -65,6 +75,7 @@ impl Default for WriteOptions {
         WriteOptions {
             sync: false,
             no_slowdown: false,
+            disable_wal: false,
         }
     }
 }
diff --git a/components/engine_traits/src/raft_engine.rs b/components/engine_traits/src/raft_engine.rs
index 0325bd397..67930ddf4 100644
--- a/components/engine_traits/src/raft_engine.rs
+++ b/components/engine_traits/src/raft_engine.rs
@@ -1,7 +1,7 @@
 // Copyright 2021 TiKV Project Authors. Licensed under Apache-2.0.
 
 use crate::*;
-use kvproto::raft_serverpb::RaftLocalState;
+use kvproto::raft_serverpb::{RaftLocalState, RaftApplyState, RegionLocalState};
 use raft::eraftpb::Entry;
 
 pub trait RaftEngineReadOnly: Sync + Send + 'static {
@@ -65,6 +65,14 @@ pub trait RaftEngine: RaftEngineReadOnly + Clone + Sync + Send + 'static {
 
     fn put_raft_state(&self, raft_group_id: u64, state: &RaftLocalState) -> Result<()>;
 
+    fn put_raft_apply_state(&self, id: u64, state: &RaftApplyState) -> Result<()>;
+
+    fn put_raft_region_state(&self, id: u64, state: &RegionLocalState) -> Result<()>;
+
+    fn get_raft_apply_state(&self, id: u64) -> Result<Option<RaftApplyState>>;
+
+    fn get_raft_region_state(&self, id: u64) -> Result<Option<RegionLocalState>>;
+
     /// Like `cut_logs` but the range could be very large. Return the deleted count.
     /// Generally, `from` can be passed in `0`.
     fn gc(&self, raft_group_id: u64, from: u64, to: u64) -> Result<usize>;
@@ -111,6 +119,10 @@ pub trait RaftLogBatch: Send {
 
     fn put_raft_state(&mut self, raft_group_id: u64, state: &RaftLocalState) -> Result<()>;
 
+    fn put_raft_apply_state(&mut self, id: u64, state: &RaftApplyState) -> Result<()>;
+
+    fn put_raft_region_state(&mut self, id: u64, state: &RegionLocalState) -> Result<()>;
+
     /// The data size of this RaftLogBatch.
     fn persist_size(&self) -> usize;
 
diff --git a/components/raft_log_engine/src/engine.rs b/components/raft_log_engine/src/engine.rs
index f26d31011..097de35c6 100644
--- a/components/raft_log_engine/src/engine.rs
+++ b/components/raft_log_engine/src/engine.rs
@@ -4,14 +4,13 @@ use std::fs;
 use std::path::Path;
 
 use engine_traits::{
-    CacheStats, RaftEngine, RaftEngineReadOnly, RaftLogBatch as RaftLogBatchTrait, Result,
+    CacheStats, RaftEngine, RaftEngineReadOnly, RaftLogBatch as RaftLogBatchTrait, Result, 
 };
-use kvproto::raft_serverpb::RaftLocalState;
+use kvproto::raft_serverpb::{RaftLocalState, RaftApplyState, RegionLocalState};
 use raft::eraftpb::Entry;
 use raft_engine::{
     Command, Error as RaftEngineError, LogBatch, MessageExt, RaftLogEngine as RawRaftEngine,
 };
-
 pub use raft_engine::{Config as RaftEngineConfig, RecoveryMode};
 
 #[derive(Clone)]
@@ -61,6 +60,8 @@ impl RaftLogEngine {
 pub struct RaftLogBatch(LogBatch<MessageExtTyped>);
 
 const RAFT_LOG_STATE_KEY: &[u8] = b"R";
+const RAFT_REGION_STATE_KEY: &[u8] = &[0x03];
+const RAFT_APPLY_STATE_KEY: &[u8] = &[0x04];
 
 impl RaftLogBatchTrait for RaftLogBatch {
     fn append(&mut self, raft_group_id: u64, entries: Vec<Entry>) -> Result<()> {
@@ -78,6 +79,18 @@ impl RaftLogBatchTrait for RaftLogBatch {
             .map_err(transfer_error)
     }
 
+    fn put_raft_apply_state(&mut self, id: u64, state: &RaftApplyState) -> Result<()> {
+        self.0
+            .put_message(id, RAFT_APPLY_STATE_KEY.to_vec(), state)
+            .map_err(transfer_error)
+    }
+
+    fn put_raft_region_state(&mut self, id: u64, state: &RegionLocalState) -> Result<()> {
+        self.0
+            .put_message(id, RAFT_REGION_STATE_KEY.to_vec(), state)
+            .map_err(transfer_error)
+    }
+
     fn persist_size(&self) -> usize {
         self.0.approximate_size()
     }
@@ -175,6 +188,30 @@ impl RaftEngine for RaftLogEngine {
             .map_err(transfer_error)
     }
 
+    fn put_raft_apply_state(&self, id: u64, state: &RaftApplyState) -> Result<()> {
+        self.0
+            .put_message(id, RAFT_LOG_STATE_KEY, state)
+            .map_err(transfer_error)
+    }
+
+    fn put_raft_region_state(&self, id: u64, state: &RegionLocalState) -> Result<()> {
+        self.0
+            .put_message(id, RAFT_REGION_STATE_KEY, state)
+            .map_err(transfer_error)
+    }
+
+    fn get_raft_apply_state(&self, id: u64) -> Result<Option<RaftApplyState>> {
+        self.0
+            .get_message(id, RAFT_APPLY_STATE_KEY)
+            .map_err(transfer_error)
+    }
+
+    fn get_raft_region_state(&self, id: u64) -> Result<Option<RegionLocalState>> {
+        self.0
+            .get_message(id, RAFT_REGION_STATE_KEY)
+            .map_err(transfer_error)
+    }
+
     fn gc(&self, raft_group_id: u64, _from: u64, to: u64) -> Result<usize> {
         Ok(self.0.compact_to(raft_group_id, to) as usize)
     }
diff --git a/components/raftstore/src/store/async_io/write.rs b/components/raftstore/src/store/async_io/write.rs
index 3cad29223..9fa43df5f 100644
--- a/components/raftstore/src/store/async_io/write.rs
+++ b/components/raftstore/src/store/async_io/write.rs
@@ -484,7 +484,10 @@ where
             raft_before_save_kv_on_store_3();
             let now = Instant::now();
             let mut write_opts = WriteOptions::new();
-            write_opts.set_sync(true);
+            // shawgerj
+            write_opts.set_sync(false);
+      	    write_opts.set_disable_wal(true);
+            //write_opts.set_sync(true);
             // TODO: Add perf context
             self.batch.kv_wb.write_opt(&write_opts).unwrap_or_else(|e| {
                 panic!(
@@ -719,7 +722,9 @@ where
     batch.before_write_to_db(&StoreWriteMetrics::new(false));
     if !batch.kv_wb.is_empty() {
         let mut write_opts = WriteOptions::new();
-        write_opts.set_sync(true);
+        // shawgerj
+        write_opts.set_sync(false);
+        write_opts.set_disable_wal(true);
         batch.kv_wb.write_opt(&write_opts).unwrap_or_else(|e| {
             panic!("test failed to write to kv engine: {:?}", e);
         });
diff --git a/components/raftstore/src/store/bootstrap.rs b/components/raftstore/src/store/bootstrap.rs
index e49efdb01..019ff4f66 100644
--- a/components/raftstore/src/store/bootstrap.rs
+++ b/components/raftstore/src/store/bootstrap.rs
@@ -79,9 +79,15 @@ pub fn prepare_bootstrap_cluster(
     box_try!(wb.put_msg(keys::PREPARE_BOOTSTRAP_KEY, region));
     box_try!(wb.put_msg_cf(CF_RAFT, &keys::region_state_key(region.get_id()), &state));
     write_initial_apply_state(&mut wb, region.get_id())?;
-    wb.write()?;
-    engines.sync_kv()?;
-
+    let mut write_opts = engine_traits::WriteOptions::new();
+    write_opts.set_sync(false);
+    write_opts.set_disable_wal(true);
+    wb.write_opt(&write_opts)?;
+    // shawgerj: flush required?
+    engines.kv.flush_all().unwrap_or_else(|e| {
+        panic!("failed to flush kv in prepare_bootstrap_cluster: {:?}", e);
+    });
+    
     let mut raft_wb = engines.raft.log_batch(1024);
     write_initial_raft_state(&mut raft_wb, region.get_id())?;
     box_try!(engines.raft.consume(&mut raft_wb, true));
diff --git a/components/raftstore/src/store/fsm/apply.rs b/components/raftstore/src/store/fsm/apply.rs
index 3c21ac288..092ac22bc 100644
--- a/components/raftstore/src/store/fsm/apply.rs
+++ b/components/raftstore/src/store/fsm/apply.rs
@@ -26,8 +26,7 @@ use crossbeam::channel::{TryRecvError, TrySendError};
 use engine_traits::PerfContext;
 use engine_traits::PerfContextKind;
 use engine_traits::{
-    DeleteStrategy, KvEngine, RaftEngine, RaftEngineReadOnly, Range as EngineRange, Snapshot,
-    WriteBatch,
+    Engines, DeleteStrategy, KvEngine, Mutable, RaftEngine, RaftEngineReadOnly, Range as EngineRange, Snapshot, WriteBatch,
 };
 use engine_traits::{SSTMetaInfo, ALL_CFS, CF_DEFAULT, CF_LOCK, CF_RAFT, CF_WRITE};
 use fail::fail_point;
@@ -83,6 +82,7 @@ use super::metrics::*;
 
 const DEFAULT_APPLY_WB_SIZE: usize = 4 * 1024;
 const APPLY_WB_SHRINK_SIZE: usize = 1024 * 1024;
+const RAFT_WB_DEFAULT_SIZE: usize = 256 * 1024;
 const SHRINK_PENDING_CMD_QUEUE_CAP: usize = 64;
 const MAX_APPLY_BATCH_SIZE: usize = 64 * 1024 * 1024;
 
@@ -342,25 +342,25 @@ pub trait Notifier<EK: KvEngine>: Send {
     fn clone_box(&self) -> Box<dyn Notifier<EK>>;
 }
 
-struct ApplyContext<EK, W>
+struct ApplyContext<EK, ER>
 where
     EK: KvEngine,
-    W: WriteBatch<EK>,
+    ER: RaftEngine,
 {
     tag: String,
     timer: Option<Instant>,
     host: CoprocessorHost<EK>,
     importer: Arc<SSTImporter>,
     region_scheduler: Scheduler<RegionTask<EK::Snapshot>>,
-    router: ApplyRouter<EK>,
+    router: ApplyRouter<EK, ER>,
     notifier: Box<dyn Notifier<EK>>,
-    engine: EK,
+    engines: Engines<EK, ER>,
     applied_batch: ApplyCallbackBatch<EK::Snapshot>,
     apply_res: Vec<ApplyRes<EK::Snapshot>>,
     exec_log_index: u64,
     exec_log_term: u64,
 
-    kv_wb: W,
+    kv_wb: EK::WriteBatch,
     kv_wb_last_bytes: u64,
     kv_wb_last_keys: u64,
 
@@ -404,27 +404,28 @@ where
     key_buffer: Vec<u8>,
 }
 
-impl<EK, W> ApplyContext<EK, W>
+impl<EK, ER> ApplyContext<EK, ER>
 where
     EK: KvEngine,
-    W: WriteBatch<EK>,
+    ER: RaftEngine,
 {
     pub fn new(
         tag: String,
         host: CoprocessorHost<EK>,
         importer: Arc<SSTImporter>,
         region_scheduler: Scheduler<RegionTask<EK::Snapshot>>,
-        engine: EK,
-        router: ApplyRouter<EK>,
+        engines: Engines<EK, ER>,
+        router: ApplyRouter<EK, ER>,
         notifier: Box<dyn Notifier<EK>>,
         cfg: &Config,
         store_id: u64,
         pending_create_peers: Arc<Mutex<HashMap<u64, (u64, bool)>>>,
         priority: Priority,
-    ) -> ApplyContext<EK, W> {
+    ) -> ApplyContext<EK, ER> {
         // If `enable_multi_batch_write` was set true, we create `RocksWriteBatchVec`.
         // Otherwise create `RocksWriteBatch`.
-        let kv_wb = W::with_capacity(&engine, DEFAULT_APPLY_WB_SIZE);
+        let kv_wb = engines.kv.write_batch_with_cap(DEFAULT_APPLY_WB_SIZE);
+//        let r_wb = engines.raft.log_batch(RAFT_WB_DEFAULT_SIZE);
 
         ApplyContext {
             tag,
@@ -432,10 +433,11 @@ where
             host,
             importer,
             region_scheduler,
-            engine: engine.clone(),
+            engines: engines.clone(),
             router,
             notifier,
             kv_wb,
+//            r_wb,
             applied_batch: ApplyCallbackBatch::new(),
             apply_res: vec![],
             exec_log_index: 0,
@@ -445,7 +447,7 @@ where
             committed_count: 0,
             sync_log_hint: false,
             use_delete_range: cfg.use_delete_range,
-            perf_context: engine.get_perf_context(cfg.perf_level, PerfContextKind::RaftstoreApply),
+            perf_context: engines.kv.get_perf_context(cfg.perf_level, PerfContextKind::RaftstoreApply),
             yield_duration: cfg.apply_yield_duration.0,
             delete_ssts: vec![],
             store_id,
@@ -465,7 +467,7 @@ where
     /// A general apply progress for a delegate is:
     /// `prepare_for` -> `commit` [-> `commit` ...] -> `finish_for`.
     /// After all delegates are handled, `write_to_db` method should be called.
-    pub fn prepare_for(&mut self, delegate: &mut ApplyDelegate<EK>) {
+    pub fn prepare_for(&mut self, delegate: &mut ApplyDelegate<EK, ER>) {
         self.applied_batch
             .push_batch(&delegate.observe_info, delegate.region.get_id());
     }
@@ -474,14 +476,14 @@ where
     /// write the changes into rocksdb.
     ///
     /// This call is valid only when it's between a `prepare_for` and `finish_for`.
-    pub fn commit(&mut self, delegate: &mut ApplyDelegate<EK>) {
+    pub fn commit(&mut self, delegate: &mut ApplyDelegate<EK, ER>) {
         if delegate.last_flush_applied_index < delegate.apply_state.get_applied_index() {
-            delegate.write_apply_state(self.kv_wb_mut());
+            delegate.write_apply_state(&mut self.kv_wb);
         }
         self.commit_opt(delegate, true);
     }
 
-    fn commit_opt(&mut self, delegate: &mut ApplyDelegate<EK>, persistent: bool) {
+    fn commit_opt(&mut self, delegate: &mut ApplyDelegate<EK, ER>, persistent: bool) {
         delegate.update_metrics(self);
         if persistent {
             self.write_to_db();
@@ -495,6 +497,11 @@ where
     /// Writes all the changes into RocksDB.
     /// If it returns true, all pending writes are persisted in engines.
     pub fn write_to_db(&mut self) -> bool {
+        fail_point!(
+            "write_to_db_begin",
+            |_| false
+        );
+        
         let need_sync = self.sync_log_hint;
         // There may be put and delete requests after ingest request in the same fsm.
         // To guarantee the correct order, we must ingest the pending_sst first, and
@@ -502,7 +509,7 @@ where
         if !self.pending_ssts.is_empty() {
             let tag = self.tag.clone();
             self.importer
-                .ingest(&self.pending_ssts, &self.engine)
+                .ingest(&self.pending_ssts, &self.engines.kv)
                 .unwrap_or_else(|e| {
                     panic!(
                         "{} failed to ingest ssts {:?}: {:?}",
@@ -511,9 +518,17 @@ where
                 });
             self.pending_ssts = vec![];
         }
+        fail_point!(
+            "write_to_db_after_ingest_sst",
+            |_| false
+        );
+
         if !self.kv_wb_mut().is_empty() {
             let mut write_opts = engine_traits::WriteOptions::new();
-            write_opts.set_sync(need_sync);
+            //write_opts.set_sync(need_sync);
+            // shawgerj
+            write_opts.set_sync(false);
+            write_opts.set_disable_wal(true);
             self.kv_wb().write_opt(&write_opts).unwrap_or_else(|e| {
                 panic!("failed to write to engine: {:?}", e);
             });
@@ -523,7 +538,7 @@ where
             if data_size > APPLY_WB_SHRINK_SIZE {
                 // Control the memory usage for the WriteBatch. Whether it's `RocksWriteBatch` or
                 // `RocksWriteBatchVec` depends on the `enable_multi_batch_write` configuration.
-                self.kv_wb = W::with_capacity(&self.engine, DEFAULT_APPLY_WB_SIZE);
+                self.kv_wb = self.engines.kv.write_batch_with_cap(DEFAULT_APPLY_WB_SIZE);
             } else {
                 // Clear data, reuse the WriteBatch, this can reduce memory allocations and deallocations.
                 self.kv_wb_mut().clear();
@@ -531,6 +546,24 @@ where
             self.kv_wb_last_bytes = 0;
             self.kv_wb_last_keys = 0;
         }
+        fail_point!(
+            "write_to_db_after_write_wb",
+            |_| false
+        );
+
+        // need_sync is only true when admin commands (except gc) or ingest sst
+        // are processed. Not the common case. We don't have a WAL to sync, so
+        // flush the memtables...
+        if need_sync {
+            self.engines.kv.flush_all().unwrap_or_else(|e| {
+                panic!("failed to flush kv in apply write_to_db: {:?}", e);
+            });
+        }
+        fail_point!(
+            "write_to_db_after_sync_memtables",
+            |_| false
+        );
+
         if !self.delete_ssts.is_empty() {
             let tag = self.tag.clone();
             for sst in self.delete_ssts.drain(..) {
@@ -539,6 +572,11 @@ where
                 });
             }
         }
+        fail_point!(
+            "write_to_db_after_delete_ssts",
+            |_| false
+        );
+        
         // Take the applied commands and their callback
         let ApplyCallbackBatch {
             cmd_batch,
@@ -547,7 +585,7 @@ where
         } = mem::replace(&mut self.applied_batch, ApplyCallbackBatch::new());
         // Call it before invoking callback for preventing Commit is executed before Prewrite is observed.
         self.host
-            .on_flush_applied_cmd_batch(batch_max_level, cmd_batch, &self.engine);
+            .on_flush_applied_cmd_batch(batch_max_level, cmd_batch, &self.engines.kv);
         // Invoke callbacks
         let now = Instant::now();
         for (cb, resp) in cb_batch.drain(..) {
@@ -567,11 +605,11 @@ where
     /// Finishes `Apply`s for the delegate.
     pub fn finish_for(
         &mut self,
-        delegate: &mut ApplyDelegate<EK>,
+        delegate: &mut ApplyDelegate<EK, ER>,
         results: VecDeque<ExecResult<EK::Snapshot>>,
     ) {
         if !delegate.pending_remove {
-            delegate.write_apply_state(self.kv_wb_mut());
+            delegate.write_apply_state(&mut self.kv_wb);
         }
         self.commit_opt(delegate, false);
         self.apply_res.push(ApplyRes {
@@ -592,15 +630,20 @@ where
     }
 
     #[inline]
-    pub fn kv_wb(&self) -> &W {
+    pub fn kv_wb(&self) -> &EK::WriteBatch {
         &self.kv_wb
     }
 
     #[inline]
-    pub fn kv_wb_mut(&mut self) -> &mut W {
+    pub fn kv_wb_mut(&mut self) -> &mut EK::WriteBatch {
         &mut self.kv_wb
     }
 
+    // #[inline]
+    // pub fn r_wb_mut(&mut self) -> &mut ER::LogBatch {
+    //     &mut self.r_wb
+    // }
+    
     /// Flush all pending writes to engines.
     /// If it returns true, all pending writes are persisted in engines.
     pub fn flush(&mut self) -> bool {
@@ -825,9 +868,10 @@ pub struct NewSplitPeer {
 /// handle the apply task to make the code logic more clear.
 #[derive(Derivative)]
 #[derivative(Debug)]
-pub struct ApplyDelegate<EK>
+pub struct ApplyDelegate<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
     /// The ID of the peer.
     id: u64,
@@ -889,13 +933,15 @@ where
     raft_engine: Box<dyn RaftEngineReadOnly>,
 
     trace: ApplyMemoryTrace,
+    _phantom: PhantomData<ER>,
 }
 
-impl<EK> ApplyDelegate<EK>
+impl<EK, ER> ApplyDelegate<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
-    fn from_registration(reg: Registration) -> ApplyDelegate<EK> {
+    fn from_registration(reg: Registration) -> ApplyDelegate<EK, ER> {
         ApplyDelegate {
             id: reg.id,
             tag: format!("[region {}] {}", reg.region.get_id(), reg.id),
@@ -920,6 +966,7 @@ where
             priority: Priority::Normal,
             raft_engine: reg.raft_engine,
             trace: ApplyMemoryTrace::default(),
+            _phantom: PhantomData,
         }
     }
 
@@ -932,9 +979,9 @@ where
     }
 
     /// Handles all the committed_entries, namely, applies the committed entries.
-    fn handle_raft_committed_entries<W: WriteBatch<EK>>(
+    fn handle_raft_committed_entries(
         &mut self,
-        apply_ctx: &mut ApplyContext<EK, W>,
+        apply_ctx: &mut ApplyContext<EK, ER>,
         mut committed_entries_drainer: Drain<Entry>,
     ) {
         if committed_entries_drainer.len() == 0 {
@@ -1003,28 +1050,29 @@ where
         }
     }
 
-    fn update_metrics<W: WriteBatch<EK>>(&mut self, apply_ctx: &ApplyContext<EK, W>) {
+    fn update_metrics(&mut self, apply_ctx: &ApplyContext<EK, ER>) {
         self.metrics.written_bytes += apply_ctx.delta_bytes();
         self.metrics.written_keys += apply_ctx.delta_keys();
     }
 
-    fn write_apply_state<W: WriteBatch<EK>>(&self, wb: &mut W) {
-        wb.put_msg_cf(
+    fn write_apply_state(&self, kv_rb: &mut EK::WriteBatch) {
+        // write to kv-store
+        kv_rb.put_msg_cf(
             CF_RAFT,
             &keys::apply_state_key(self.region.get_id()),
             &self.apply_state,
         )
         .unwrap_or_else(|e| {
             panic!(
-                "{} failed to save apply state to write batch, error: {:?}",
+                "{} failed to save apply state to kv write batch, error: {:?}",
                 self.tag, e
             );
         });
     }
 
-    fn handle_raft_entry_normal<W: WriteBatch<EK>>(
+    fn handle_raft_entry_normal(
         &mut self,
-        apply_ctx: &mut ApplyContext<EK, W>,
+        apply_ctx: &mut ApplyContext<EK, ER>,
         entry: &Entry,
     ) -> ApplyResult<EK::Snapshot> {
         fail_point!(
@@ -1085,9 +1133,9 @@ where
         ApplyResult::None
     }
 
-    fn handle_raft_entry_conf_change<W: WriteBatch<EK>>(
+    fn handle_raft_entry_conf_change(
         &mut self,
-        apply_ctx: &mut ApplyContext<EK, W>,
+        apply_ctx: &mut ApplyContext<EK, ER>,
         entry: &Entry,
     ) -> ApplyResult<EK::Snapshot> {
         // Although conf change can't yield in normal case, it is convenient to
@@ -1162,9 +1210,9 @@ where
         None
     }
 
-    fn process_raft_cmd<W: WriteBatch<EK>>(
+    fn process_raft_cmd(
         &mut self,
-        apply_ctx: &mut ApplyContext<EK, W>,
+        apply_ctx: &mut ApplyContext<EK, ER>,
         index: u64,
         term: u64,
         cmd: RaftCmdRequest,
@@ -1211,9 +1259,9 @@ where
     ///   2. it encounters an error that may not occur on all stores, in this case
     /// we should try to apply the entry again or panic. Considering that this
     /// usually due to disk operation fail, which is rare, so just panic is ok.
-    fn apply_raft_cmd<W: WriteBatch<EK>>(
+    fn apply_raft_cmd(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         index: u64,
         term: u64,
         req: &RaftCmdRequest,
@@ -1313,7 +1361,7 @@ where
         (resp, exec_result)
     }
 
-    fn destroy<W: WriteBatch<EK>>(&mut self, apply_ctx: &mut ApplyContext<EK, W>) {
+    fn destroy(&mut self, apply_ctx: &mut ApplyContext<EK, ER>) {
         self.stopped = true;
         apply_ctx.router.close(self.region_id());
         for cmd in self.pending_cmds.normals.drain(..) {
@@ -1351,14 +1399,15 @@ where
     }
 }
 
-impl<EK> ApplyDelegate<EK>
+impl<EK, ER> ApplyDelegate<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
     // Only errors that will also occur on all other stores should be returned.
-    fn exec_raft_cmd<W: WriteBatch<EK>>(
+    fn exec_raft_cmd(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         req: &RaftCmdRequest,
     ) -> Result<(RaftCmdResponse, ApplyResult<EK::Snapshot>)> {
         // Include region for epoch not match after merge may cause key not in range.
@@ -1372,9 +1421,9 @@ where
         }
     }
 
-    fn exec_admin_cmd<W: WriteBatch<EK>>(
+    fn exec_admin_cmd(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         req: &RaftCmdRequest,
     ) -> Result<(RaftCmdResponse, ApplyResult<EK::Snapshot>)> {
         let request = req.get_admin_request();
@@ -1416,9 +1465,9 @@ where
         Ok((resp, exec_result))
     }
 
-    fn exec_write_cmd<W: WriteBatch<EK>>(
+    fn exec_write_cmd(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         req: &RaftCmdRequest,
     ) -> Result<(RaftCmdResponse, ApplyResult<EK::Snapshot>)> {
         fail_point!(
@@ -1439,7 +1488,7 @@ where
                 CmdType::Put => self.handle_put(ctx, req),
                 CmdType::Delete => self.handle_delete(ctx, req),
                 CmdType::DeleteRange => {
-                    self.handle_delete_range(&ctx.engine, req, &mut ranges, ctx.use_delete_range)
+                    self.handle_delete_range(&ctx.engines.kv, req, &mut ranges, ctx.use_delete_range)
                 }
                 CmdType::IngestSst => self.handle_ingest_sst(ctx, req, &mut ssts),
                 // Readonly commands are handled in raftstore directly.
@@ -1491,13 +1540,14 @@ where
 }
 
 // Write commands related.
-impl<EK> ApplyDelegate<EK>
+impl<EK, ER> ApplyDelegate<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
-    fn handle_put<W: WriteBatch<EK>>(
+    fn handle_put(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         req: &Request,
     ) -> Result<()> {
         let (key, value) = (req.get_put().get_key(), req.get_put().get_value());
@@ -1541,9 +1591,9 @@ where
         Ok(())
     }
 
-    fn handle_delete<W: WriteBatch<EK>>(
+    fn handle_delete(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         req: &Request,
     ) -> Result<()> {
         let key = req.get_delete().get_key();
@@ -1660,9 +1710,9 @@ where
         Ok(())
     }
 
-    fn handle_ingest_sst<W: WriteBatch<EK>>(
+    fn handle_ingest_sst(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         req: &Request,
         ssts: &mut Vec<SSTMetaInfo>,
     ) -> Result<()> {
@@ -1721,13 +1771,14 @@ mod confchange_cmd_metric {
 }
 
 // Admin commands related.
-impl<EK> ApplyDelegate<EK>
+impl<EK, ER> ApplyDelegate<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
-    fn exec_change_peer<W: WriteBatch<EK>>(
+    fn exec_change_peer(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         request: &AdminRequest,
     ) -> Result<(AdminResponse, ApplyResult<EK::Snapshot>)> {
         assert!(request.has_change_peer());
@@ -1907,7 +1958,7 @@ where
         } else {
             PeerState::Normal
         };
-        if let Err(e) = write_peer_state(ctx.kv_wb_mut(), &region, state, None) {
+        if let Err(e) = write_peer_state(&mut ctx.kv_wb, &region, state, None) {
             panic!("{} failed to update region state: {:?}", self.tag, e);
         }
 
@@ -1925,9 +1976,9 @@ where
         ))
     }
 
-    fn exec_change_peer_v2<W: WriteBatch<EK>>(
+    fn exec_change_peer_v2(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         request: &AdminRequest,
     ) -> Result<(AdminResponse, ApplyResult<EK::Snapshot>)> {
         assert!(request.has_change_peer_v2());
@@ -1952,7 +2003,7 @@ where
             PeerState::Normal
         };
 
-        if let Err(e) = write_peer_state(ctx.kv_wb_mut(), &region, state, None) {
+        if let Err(e) = write_peer_state(&mut ctx.kv_wb, &region, state, None) {
             panic!("{} failed to update region state: {:?}", self.tag, e);
         }
 
@@ -2152,9 +2203,9 @@ where
         Ok(region)
     }
 
-    fn exec_split<W: WriteBatch<EK>>(
+    fn exec_split(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         req: &AdminRequest,
     ) -> Result<(AdminResponse, ApplyResult<EK::Snapshot>)> {
         info!(
@@ -2174,9 +2225,9 @@ where
         self.exec_batch_split(ctx, &admin_req)
     }
 
-    fn exec_batch_split<W: WriteBatch<EK>>(
+    fn exec_batch_split(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         req: &AdminRequest,
     ) -> Result<(AdminResponse, ApplyResult<EK::Snapshot>)> {
         fail_point!("apply_before_split");
@@ -2304,7 +2355,8 @@ where
         for (region_id, new_split_peer) in new_split_regions.iter_mut() {
             let region_state_key = keys::region_state_key(*region_id);
             match ctx
-                .engine
+                .engines
+                .kv
                 .get_msg_cf::<RegionLocalState>(CF_RAFT, &region_state_key)
             {
                 Ok(None) => (),
@@ -2337,7 +2389,6 @@ where
             }
         }
 
-        let kv_wb_mut = ctx.kv_wb_mut();
         for new_region in &regions {
             if new_region.get_id() == derived.get_id() {
                 continue;
@@ -2354,8 +2405,8 @@ where
                 );
                 continue;
             }
-            write_peer_state(kv_wb_mut, new_region, PeerState::Normal, None)
-                .and_then(|_| write_initial_apply_state(kv_wb_mut, new_region.get_id()))
+            write_peer_state(&mut ctx.kv_wb, new_region, PeerState::Normal, None)
+                .and_then(|_| write_initial_apply_state(&mut ctx.kv_wb, new_region.get_id()))
                 .unwrap_or_else(|e| {
                     panic!(
                         "{} fails to save split region {:?}: {:?}",
@@ -2363,7 +2414,7 @@ where
                     )
                 });
         }
-        write_peer_state(kv_wb_mut, &derived, PeerState::Normal, None).unwrap_or_else(|e| {
+        write_peer_state(&mut ctx.kv_wb, &derived, PeerState::Normal, None).unwrap_or_else(|e| {
             panic!("{} fails to update region {:?}: {:?}", self.tag, derived, e)
         });
         let mut resp = AdminResponse::default();
@@ -2386,9 +2437,9 @@ where
         ))
     }
 
-    fn exec_prepare_merge<W: WriteBatch<EK>>(
+    fn exec_prepare_merge(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         req: &AdminRequest,
     ) -> Result<(AdminResponse, ApplyResult<EK::Snapshot>)> {
         fail_point!("apply_before_prepare_merge");
@@ -2425,8 +2476,9 @@ where
         merging_state.set_min_index(index);
         merging_state.set_target(prepare_merge.get_target().to_owned());
         merging_state.set_commit(ctx.exec_log_index);
+        
         write_peer_state(
-            ctx.kv_wb_mut(),
+            &mut ctx.kv_wb,
             &region,
             PeerState::Merging,
             Some(merging_state.clone()),
@@ -2461,9 +2513,9 @@ where
     // 7.   resume `exec_commit_merge` in target apply fsm
     // 8.   `on_ready_commit_merge` in target peer fsm and send `MergeResult` to source peer fsm
     // 9.   `on_merge_result` in source peer fsm (destroy itself)
-    fn exec_commit_merge<W: WriteBatch<EK>>(
+    fn exec_commit_merge(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         req: &AdminRequest,
     ) -> Result<(AdminResponse, ApplyResult<EK::Snapshot>)> {
         {
@@ -2530,7 +2582,7 @@ where
         self.ready_source_region_id = 0;
 
         let region_state_key = keys::region_state_key(source_region_id);
-        let state: RegionLocalState = match ctx.engine.get_msg_cf(CF_RAFT, &region_state_key) {
+        let state: RegionLocalState = match ctx.engines.kv.get_msg_cf(CF_RAFT, &region_state_key) {
             Ok(Some(s)) => s,
             e => panic!(
                 "{} failed to get regions state of {:?}: {:?}",
@@ -2562,14 +2614,13 @@ where
         } else {
             region.set_start_key(source_region.get_start_key().to_vec());
         }
-        let kv_wb_mut = ctx.kv_wb_mut();
-        write_peer_state(kv_wb_mut, &region, PeerState::Normal, None)
+        write_peer_state(&mut ctx.kv_wb, &region, PeerState::Normal, None)
             .and_then(|_| {
                 // TODO: maybe all information needs to be filled?
                 let mut merging_state = MergeState::default();
                 merging_state.set_target(self.region.clone());
                 write_peer_state(
-                    kv_wb_mut,
+                    &mut ctx.kv_wb,
                     source_region,
                     PeerState::Tombstone,
                     Some(merging_state),
@@ -2595,16 +2646,16 @@ where
         ))
     }
 
-    fn exec_rollback_merge<W: WriteBatch<EK>>(
+    fn exec_rollback_merge(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         req: &AdminRequest,
     ) -> Result<(AdminResponse, ApplyResult<EK::Snapshot>)> {
         fail_point!("apply_before_rollback_merge");
 
         PEER_ADMIN_CMD_COUNTER.rollback_merge.all.inc();
         let region_state_key = keys::region_state_key(self.region_id());
-        let state: RegionLocalState = match ctx.engine.get_msg_cf(CF_RAFT, &region_state_key) {
+        let state: RegionLocalState = match ctx.engines.kv.get_msg_cf(CF_RAFT, &region_state_key) {
             Ok(Some(s)) => s,
             e => panic!("{} failed to get regions state: {:?}", self.tag, e),
         };
@@ -2620,7 +2671,7 @@ where
         let version = region.get_region_epoch().get_version();
         // Update version to avoid duplicated rollback requests.
         region.mut_region_epoch().set_version(version + 1);
-        write_peer_state(ctx.kv_wb_mut(), &region, PeerState::Normal, None).unwrap_or_else(|e| {
+        write_peer_state(&mut ctx.kv_wb, &region, PeerState::Normal, None).unwrap_or_else(|e| {
             panic!(
                 "{} failed to rollback merge {:?}: {:?}",
                 self.tag, rollback, e
@@ -2701,9 +2752,9 @@ where
         ))
     }
 
-    fn exec_compute_hash<W: WriteBatch<EK>>(
+    fn exec_compute_hash(
         &self,
-        ctx: &ApplyContext<EK, W>,
+        ctx: &ApplyContext<EK, ER>,
         req: &AdminRequest,
     ) -> Result<(AdminResponse, ApplyResult<EK::Snapshot>)> {
         let resp = AdminResponse::default();
@@ -2717,14 +2768,14 @@ where
                 // open files in rocksdb.
                 // TODO: figure out another way to do consistency check without snapshot
                 // or short life snapshot.
-                snap: ctx.engine.snapshot(),
+                snap: ctx.engines.kv.snapshot(),
             }),
         ))
     }
 
-    fn exec_verify_hash<W: WriteBatch<EK>>(
+    fn exec_verify_hash(
         &self,
-        _: &ApplyContext<EK, W>,
+        _: &ApplyContext<EK, ER>,
         req: &AdminRequest,
     ) -> Result<(AdminResponse, ApplyResult<EK::Snapshot>)> {
         let verify_req = req.get_verify_hash();
@@ -3182,27 +3233,29 @@ where
     },
 }
 
-pub struct ApplyFsm<EK>
+pub struct ApplyFsm<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
-    delegate: ApplyDelegate<EK>,
+    delegate: ApplyDelegate<EK, ER>,
     receiver: Receiver<Msg<EK>>,
-    mailbox: Option<BasicMailbox<ApplyFsm<EK>>>,
+    mailbox: Option<BasicMailbox<ApplyFsm<EK, ER>>>,
 }
 
-impl<EK> ApplyFsm<EK>
+impl<EK, ER> ApplyFsm<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
-    fn from_peer<ER: RaftEngine>(
+    fn from_peer(
         peer: &Peer<EK, ER>,
-    ) -> (LooseBoundedSender<Msg<EK>>, Box<ApplyFsm<EK>>) {
+    ) -> (LooseBoundedSender<Msg<EK>>, Box<ApplyFsm<EK, ER>>) {
         let reg = Registration::new(peer);
         ApplyFsm::from_registration(reg)
     }
 
-    fn from_registration(reg: Registration) -> (LooseBoundedSender<Msg<EK>>, Box<ApplyFsm<EK>>) {
+    fn from_registration(reg: Registration) -> (LooseBoundedSender<Msg<EK>>, Box<ApplyFsm<EK, ER>>) {
         let (tx, rx) = loose_bounded(usize::MAX);
         let delegate = ApplyDelegate::from_registration(reg);
         (
@@ -3230,9 +3283,9 @@ where
     }
 
     /// Handles apply tasks, and uses the apply delegate to handle the committed entries.
-    fn handle_apply<W: WriteBatch<EK>>(
+    fn handle_apply(
         &mut self,
-        apply_ctx: &mut ApplyContext<EK, W>,
+        apply_ctx: &mut ApplyContext<EK, ER>,
         mut apply: Apply<EK::Snapshot>,
     ) {
         if apply_ctx.timer.is_none() {
@@ -3328,7 +3381,7 @@ where
         APPLY_PROPOSAL.observe(propose_num as f64);
     }
 
-    fn destroy<W: WriteBatch<EK>>(&mut self, ctx: &mut ApplyContext<EK, W>) {
+    fn destroy(&mut self, ctx: &mut ApplyContext<EK, ER>) {
         let region_id = self.delegate.region_id();
         if ctx.apply_res.iter().any(|res| res.region_id == region_id) {
             // Flush before destroying to avoid reordering messages.
@@ -3348,7 +3401,7 @@ where
     }
 
     /// Handles peer destroy. When a peer is destroyed, the corresponding apply delegate should be removed too.
-    fn handle_destroy<W: WriteBatch<EK>>(&mut self, ctx: &mut ApplyContext<EK, W>, d: Destroy) {
+    fn handle_destroy(&mut self, ctx: &mut ApplyContext<EK, ER>, d: Destroy) {
         assert_eq!(d.region_id, self.delegate.region_id());
         if d.merge_from_snapshot {
             assert_eq!(self.delegate.stopped, false);
@@ -3368,7 +3421,7 @@ where
         }
     }
 
-    fn resume_pending<W: WriteBatch<EK>>(&mut self, ctx: &mut ApplyContext<EK, W>) {
+    fn resume_pending(&mut self, ctx: &mut ApplyContext<EK, ER>) {
         if let Some(ref state) = self.delegate.wait_merge_state {
             let source_region_id = state.logs_up_to_date.load(Ordering::SeqCst);
             if source_region_id == 0 {
@@ -3400,9 +3453,9 @@ where
         }
     }
 
-    fn logs_up_to_date_for_merge<W: WriteBatch<EK>>(
+    fn logs_up_to_date_for_merge(
         &mut self,
-        ctx: &mut ApplyContext<EK, W>,
+        ctx: &mut ApplyContext<EK, ER>,
         catch_up_logs: CatchUpLogs,
     ) {
         fail_point!("after_handle_catch_up_logs_for_merge", |_| {});
@@ -3437,9 +3490,9 @@ where
     }
 
     #[allow(unused_mut)]
-    fn handle_snapshot<W: WriteBatch<EK>>(
+    fn handle_snapshot(
         &mut self,
-        apply_ctx: &mut ApplyContext<EK, W>,
+        apply_ctx: &mut ApplyContext<EK, ER>,
         snap_task: GenSnapTask,
     ) {
         if self.delegate.pending_remove || self.delegate.stopped {
@@ -3457,7 +3510,8 @@ where
             if apply_ctx.timer.is_none() {
                 apply_ctx.timer = Some(Instant::now_coarse());
             }
-            self.delegate.write_apply_state(apply_ctx.kv_wb_mut());
+            self.delegate.write_apply_state(&mut apply_ctx.kv_wb);
+
             fail_point!(
                 "apply_on_handle_snapshot_1_1",
                 self.delegate.id == 1 && self.delegate.region_id() == 1,
@@ -3469,7 +3523,7 @@ where
         }
 
         if let Err(e) = snap_task.generate_and_schedule_snapshot::<EK>(
-            apply_ctx.engine.snapshot(),
+            apply_ctx.engines.kv.snapshot(),
             self.delegate.applied_index_term,
             self.delegate.apply_state.clone(),
             &apply_ctx.region_scheduler,
@@ -3491,9 +3545,9 @@ where
         );
     }
 
-    fn handle_change<W: WriteBatch<EK>>(
+    fn handle_change(
         &mut self,
-        apply_ctx: &mut ApplyContext<EK, W>,
+        apply_ctx: &mut ApplyContext<EK, ER>,
         cmd: ChangeObserver,
         region_epoch: RegionEpoch,
         cb: Callback<EK::Snapshot>,
@@ -3548,7 +3602,7 @@ where
                 ReadResponse {
                     response: Default::default(),
                     snapshot: Some(RegionSnapshot::from_snapshot(
-                        Arc::new(apply_ctx.engine.snapshot()),
+                        Arc::new(apply_ctx.engines.kv.snapshot()),
                         Arc::new(self.delegate.region.clone()),
                     )),
                     txn_extra_op: TxnExtraOp::Noop,
@@ -3575,9 +3629,9 @@ where
         cb.invoke_read(resp);
     }
 
-    fn handle_tasks<W: WriteBatch<EK>>(
+    fn handle_tasks(
         &mut self,
-        apply_ctx: &mut ApplyContext<EK, W>,
+        apply_ctx: &mut ApplyContext<EK, ER>,
         msgs: &mut Vec<Msg<EK>>,
     ) {
         let mut drainer = msgs.drain(..);
@@ -3647,9 +3701,10 @@ where
     }
 }
 
-impl<EK> Fsm for ApplyFsm<EK>
+impl<EK, ER> Fsm for ApplyFsm<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
     type Message = Msg<EK>;
 
@@ -3680,9 +3735,10 @@ where
     }
 }
 
-impl<EK> Drop for ApplyFsm<EK>
+impl<EK, ER> Drop for ApplyFsm<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
     fn drop(&mut self) {
         if tikv_util::thread_group::is_shutdown(!cfg!(test)) {
@@ -3728,23 +3784,23 @@ impl Fsm for ControlFsm {
     }
 }
 
-pub struct ApplyPoller<EK, W>
+pub struct ApplyPoller<EK, ER>
 where
     EK: KvEngine,
-    W: WriteBatch<EK>,
+    ER: RaftEngine,
 {
     msg_buf: Vec<Msg<EK>>,
-    apply_ctx: ApplyContext<EK, W>,
+    apply_ctx: ApplyContext<EK, ER>,
     messages_per_tick: usize,
     cfg_tracker: Tracker<Config>,
 
     trace_event: TraceEvent,
 }
 
-impl<EK, W> PollHandler<ApplyFsm<EK>, ControlFsm> for ApplyPoller<EK, W>
+impl<EK, ER> PollHandler<ApplyFsm<EK, ER>, ControlFsm> for ApplyPoller<EK, ER>
 where
     EK: KvEngine,
-    W: WriteBatch<EK>,
+    ER: RaftEngine,
 {
     fn begin(&mut self, _batch_size: usize) {
         if let Some(incoming) = self.cfg_tracker.any_new() {
@@ -3787,7 +3843,7 @@ where
         }
     }
 
-    fn handle_normal(&mut self, normal: &mut impl DerefMut<Target = ApplyFsm<EK>>) -> HandleResult {
+    fn handle_normal(&mut self, normal: &mut impl DerefMut<Target = ApplyFsm<EK, ER>>) -> HandleResult {
         let mut handle_result = HandleResult::KeepProcessing;
         normal.delegate.handle_start = Some(Instant::now_coarse());
         if normal.delegate.yield_state.is_some() {
@@ -3846,7 +3902,7 @@ where
         handle_result
     }
 
-    fn end(&mut self, fsms: &mut [Option<impl DerefMut<Target = ApplyFsm<EK>>>]) {
+    fn end(&mut self, fsms: &mut [Option<impl DerefMut<Target = ApplyFsm<EK, ER>>>]) {
         self.apply_ctx.flush();
         for fsm in fsms.iter_mut().flatten() {
             fsm.delegate.last_flush_applied_index = fsm.delegate.apply_state.get_applied_index();
@@ -3860,36 +3916,33 @@ where
     }
 }
 
-pub struct Builder<EK: KvEngine, W: WriteBatch<EK>> {
+pub struct Builder<EK: KvEngine, ER: RaftEngine> {
     tag: String,
     cfg: Arc<VersionTrack<Config>>,
     coprocessor_host: CoprocessorHost<EK>,
     importer: Arc<SSTImporter>,
     region_scheduler: Scheduler<RegionTask<<EK as KvEngine>::Snapshot>>,
-    engine: EK,
+    engines: Engines<EK, ER>,
     sender: Box<dyn Notifier<EK>>,
-    router: ApplyRouter<EK>,
-    _phantom: PhantomData<W>,
+    router: ApplyRouter<EK, ER>,
+    _phantom: PhantomData<EK::WriteBatch>,
     store_id: u64,
     pending_create_peers: Arc<Mutex<HashMap<u64, (u64, bool)>>>,
 }
 
-impl<EK: KvEngine, W> Builder<EK, W>
-where
-    W: WriteBatch<EK>,
-{
-    pub fn new<T, ER: RaftEngine>(
+impl<EK: KvEngine, ER: RaftEngine> Builder<EK, ER> {
+    pub fn new<T>(
         builder: &RaftPollerBuilder<EK, ER, T>,
         sender: Box<dyn Notifier<EK>>,
-        router: ApplyRouter<EK>,
-    ) -> Builder<EK, W> {
+        router: ApplyRouter<EK, ER>,
+    ) -> Builder<EK, ER> {
         Builder {
             tag: format!("[store {}]", builder.store.get_id()),
             cfg: builder.cfg.clone(),
             coprocessor_host: builder.coprocessor_host.clone(),
             importer: builder.importer.clone(),
             region_scheduler: builder.region_scheduler.clone(),
-            engine: builder.engines.kv.clone(),
+            engines: builder.engines.clone(),
             _phantom: PhantomData,
             sender,
             router,
@@ -3899,14 +3952,14 @@ where
     }
 }
 
-impl<EK, W> HandlerBuilder<ApplyFsm<EK>, ControlFsm> for Builder<EK, W>
+impl<EK, ER> HandlerBuilder<ApplyFsm<EK, ER>, ControlFsm> for Builder<EK, ER>
 where
     EK: KvEngine,
-    W: WriteBatch<EK>,
+    ER: RaftEngine,
 {
-    type Handler = ApplyPoller<EK, W>;
+    type Handler = ApplyPoller<EK, ER>;
 
-    fn build(&mut self, priority: Priority) -> ApplyPoller<EK, W> {
+    fn build(&mut self, priority: Priority) -> ApplyPoller<EK, ER> {
         let cfg = self.cfg.value();
         ApplyPoller {
             msg_buf: Vec::with_capacity(cfg.messages_per_tick),
@@ -3915,7 +3968,7 @@ where
                 self.coprocessor_host.clone(),
                 self.importer.clone(),
                 self.region_scheduler.clone(),
-                self.engine.clone(),
+                self.engines.clone(),
                 self.router.clone(),
                 self.sender.clone_box(),
                 &cfg,
@@ -3931,36 +3984,40 @@ where
 }
 
 #[derive(Clone)]
-pub struct ApplyRouter<EK>
+pub struct ApplyRouter<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
-    pub router: BatchRouter<ApplyFsm<EK>, ControlFsm>,
+    pub router: BatchRouter<ApplyFsm<EK, ER>, ControlFsm>,
 }
 
-impl<EK> Deref for ApplyRouter<EK>
+impl<EK, ER> Deref for ApplyRouter<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
-    type Target = BatchRouter<ApplyFsm<EK>, ControlFsm>;
+    type Target = BatchRouter<ApplyFsm<EK, ER>, ControlFsm>;
 
-    fn deref(&self) -> &BatchRouter<ApplyFsm<EK>, ControlFsm> {
+    fn deref(&self) -> &BatchRouter<ApplyFsm<EK, ER>, ControlFsm> {
         &self.router
     }
 }
 
-impl<EK> DerefMut for ApplyRouter<EK>
+impl<EK, ER> DerefMut for ApplyRouter<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
-    fn deref_mut(&mut self) -> &mut BatchRouter<ApplyFsm<EK>, ControlFsm> {
+    fn deref_mut(&mut self) -> &mut BatchRouter<ApplyFsm<EK, ER>, ControlFsm> {
         &mut self.router
     }
 }
 
-impl<EK> ApplyRouter<EK>
+impl<EK, ER> ApplyRouter<EK, ER>
 where
     EK: KvEngine,
+    ER: RaftEngine,
 {
     pub fn schedule_task(&self, region_id: u64, msg: Msg<EK>) {
         let reg = match self.try_send(region_id, msg) {
@@ -4043,12 +4100,12 @@ where
         self.register(region_id, mailbox);
     }
 
-    pub fn register(&self, region_id: u64, mailbox: BasicMailbox<ApplyFsm<EK>>) {
+    pub fn register(&self, region_id: u64, mailbox: BasicMailbox<ApplyFsm<EK, ER>>) {
         self.router.register(region_id, mailbox);
         self.update_trace();
     }
 
-    pub fn register_all(&self, mailboxes: Vec<(u64, BasicMailbox<ApplyFsm<EK>>)>) {
+    pub fn register_all(&self, mailboxes: Vec<(u64, BasicMailbox<ApplyFsm<EK, ER>>)>) {
         self.router.register_all(mailboxes);
         self.update_trace();
     }
@@ -4065,26 +4122,26 @@ where
     }
 }
 
-pub struct ApplyBatchSystem<EK: KvEngine> {
-    system: BatchSystem<ApplyFsm<EK>, ControlFsm>,
+pub struct ApplyBatchSystem<EK: KvEngine, ER: RaftEngine> {
+    system: BatchSystem<ApplyFsm<EK, ER>, ControlFsm>,
 }
 
-impl<EK: KvEngine> Deref for ApplyBatchSystem<EK> {
-    type Target = BatchSystem<ApplyFsm<EK>, ControlFsm>;
+impl<EK: KvEngine, ER: RaftEngine> Deref for ApplyBatchSystem<EK, ER> {
+    type Target = BatchSystem<ApplyFsm<EK, ER>, ControlFsm>;
 
-    fn deref(&self) -> &BatchSystem<ApplyFsm<EK>, ControlFsm> {
+    fn deref(&self) -> &BatchSystem<ApplyFsm<EK, ER>, ControlFsm> {
         &self.system
     }
 }
 
-impl<EK: KvEngine> DerefMut for ApplyBatchSystem<EK> {
-    fn deref_mut(&mut self) -> &mut BatchSystem<ApplyFsm<EK>, ControlFsm> {
+impl<EK: KvEngine, ER: RaftEngine> DerefMut for ApplyBatchSystem<EK, ER> {
+    fn deref_mut(&mut self) -> &mut BatchSystem<ApplyFsm<EK, ER>, ControlFsm> {
         &mut self.system
     }
 }
 
-impl<EK: KvEngine> ApplyBatchSystem<EK> {
-    pub fn schedule_all<'a, ER: RaftEngine>(&self, peers: impl Iterator<Item = &'a Peer<EK, ER>>) {
+impl<EK: KvEngine, ER: RaftEngine> ApplyBatchSystem<EK, ER> {
+    pub fn schedule_all<'a>(&self, peers: impl Iterator<Item = &'a Peer<EK, ER>>) {
         let mut mailboxes = Vec::with_capacity(peers.size_hint().0);
         for peer in peers {
             let (tx, fsm) = ApplyFsm::from_peer(peer);
@@ -4097,9 +4154,9 @@ impl<EK: KvEngine> ApplyBatchSystem<EK> {
     }
 }
 
-pub fn create_apply_batch_system<EK: KvEngine>(
+pub fn create_apply_batch_system<EK: KvEngine, ER: RaftEngine>(
     cfg: &Config,
-) -> (ApplyRouter<EK>, ApplyBatchSystem<EK>) {
+) -> (ApplyRouter<EK, ER>, ApplyBatchSystem<EK, ER>) {
     let (control_tx, control_fsm) = ControlFsm::new();
     let (router, system) =
         batch_system::create_system(&cfg.apply_batch_system, control_tx, control_fsm);
@@ -4193,8 +4250,9 @@ mod tests {
     use crate::store::peer_storage::RAFT_INIT_LOG_INDEX;
     use crate::store::util::{new_learner_peer, new_peer};
     use engine_panic::PanicEngine;
-    use engine_test::kv::{new_engine, KvTestEngine, KvTestSnapshot, KvTestWriteBatch};
-    use engine_traits::{Peekable as PeekableTrait, WriteBatchExt};
+    use engine_test::kv::{KvTestEngine, KvTestSnapshot, KvTestWriteBatch};
+    use engine_test::raft::RaftTestEngine;
+    use engine_traits::{Engines, Peekable as PeekableTrait, WriteBatchExt};
     use kvproto::metapb::{self, RegionEpoch};
     use kvproto::raft_cmdpb::*;
     use protobuf::Message;
@@ -4217,16 +4275,22 @@ mod tests {
         }
     }
 
-    pub fn create_tmp_engine(path: &str) -> (TempDir, KvTestEngine) {
+    pub fn create_tmp_engines(path: &str) -> (TempDir, Engines<KvTestEngine, RaftTestEngine>) {
         let path = Builder::new().prefix(path).tempdir().unwrap();
-        let engine = new_engine(
+        let kv_engine = engine_test::kv::new_engine(
             path.path().join("db").to_str().unwrap(),
             None,
             ALL_CFS,
             None,
-        )
-        .unwrap();
-        (path, engine)
+        ).unwrap();
+        let raft_engine = engine_test::raft::new_engine(
+            path.path().join("raft").to_str().unwrap(),
+            None,
+            CF_DEFAULT,
+            None,
+        ).unwrap();
+        let engines = Engines::new(kv_engine, raft_engine);
+        (path, engines)
     }
 
     pub fn create_tmp_importer(path: &str) -> (TempDir, Arc<SSTImporter>) {
@@ -4368,10 +4432,11 @@ mod tests {
         assert_eq!(has_high_latency_operation(&cmd), true);
     }
 
-    fn validate<F, E>(router: &ApplyRouter<E>, region_id: u64, validate: F)
+    fn validate<F, E, D>(router: &ApplyRouter<E, D>, region_id: u64, validate: F)
     where
-        F: FnOnce(&ApplyDelegate<E>) + Send + 'static,
+        F: FnOnce(&ApplyDelegate<E, D>) + Send + 'static,
         E: KvEngine,
+        D: RaftEngine,
     {
         let (validate_tx, validate_rx) = mpsc::channel();
         router.schedule_task(
@@ -4379,7 +4444,7 @@ mod tests {
             Msg::Validate(
                 region_id,
                 Box::new(move |delegate: *const u8| {
-                    let delegate = unsafe { &*(delegate as *const ApplyDelegate<E>) };
+                    let delegate = unsafe { &*(delegate as *const ApplyDelegate<E, D>) };
                     validate(delegate);
                     validate_tx.send(()).unwrap();
                 }),
@@ -4389,9 +4454,10 @@ mod tests {
     }
 
     // Make sure msgs are handled in the same batch.
-    fn batch_messages<E>(router: &ApplyRouter<E>, region_id: u64, msgs: Vec<Msg<E>>)
+    fn batch_messages<E, D>(router: &ApplyRouter<E, D>, region_id: u64, msgs: Vec<Msg<E>>)
     where
         E: KvEngine,
+        D: RaftEngine,
     {
         let (notify1, wait1) = mpsc::channel();
         let (notify2, wait2) = mpsc::channel();
@@ -4471,20 +4537,20 @@ mod tests {
     fn test_basic_flow() {
         let (tx, rx) = mpsc::channel();
         let sender = Box::new(TestNotifier { tx });
-        let (_tmp, engine) = create_tmp_engine("apply-basic");
+        let (_tmp, engines) = create_tmp_engines("apply-basic");
         let (_dir, importer) = create_tmp_importer("apply-basic");
         let (region_scheduler, mut snapshot_rx) = dummy_scheduler();
         let cfg = Arc::new(VersionTrack::new(Config::default()));
         let (router, mut system) = create_apply_batch_system(&cfg.value());
         let pending_create_peers = Arc::new(Mutex::new(HashMap::default()));
-        let builder = super::Builder::<KvTestEngine, KvTestWriteBatch> {
+        let builder = super::Builder::<KvTestEngine, RaftTestEngine> {
             tag: "test-store".to_owned(),
             cfg,
             coprocessor_host: CoprocessorHost::<KvTestEngine>::default(),
             importer,
             region_scheduler,
             sender,
-            engine,
+            engines,
             router: router.clone(),
             _phantom: Default::default(),
             store_id: 1,
@@ -4800,7 +4866,7 @@ mod tests {
 
     #[test]
     fn test_handle_raft_committed_entries() {
-        let (_path, engine) = create_tmp_engine("test-delegate");
+        let (_path, engines) = create_tmp_engines("test-delegate");
         let (import_dir, importer) = create_tmp_importer("test-delegate");
         let obs = ApplyObserver::default();
         let mut host = CoprocessorHost::<KvTestEngine>::default();
@@ -4813,14 +4879,14 @@ mod tests {
         let cfg = Arc::new(VersionTrack::new(Config::default()));
         let (router, mut system) = create_apply_batch_system(&cfg.value());
         let pending_create_peers = Arc::new(Mutex::new(HashMap::default()));
-        let builder = super::Builder::<KvTestEngine, KvTestWriteBatch> {
+        let builder = super::Builder::<KvTestEngine, RaftTestEngine> {
             tag: "test-store".to_owned(),
             cfg,
             sender,
             region_scheduler,
             coprocessor_host: host,
             importer: importer.clone(),
-            engine: engine.clone(),
+            engines: engines.clone(),
             router: router.clone(),
             _phantom: Default::default(),
             store_id: 1,
@@ -4862,9 +4928,9 @@ mod tests {
         let dk_k1 = keys::data_key(b"k1");
         let dk_k2 = keys::data_key(b"k2");
         let dk_k3 = keys::data_key(b"k3");
-        assert_eq!(engine.get_value(&dk_k1).unwrap().unwrap(), b"v1");
-        assert_eq!(engine.get_value(&dk_k2).unwrap().unwrap(), b"v1");
-        assert_eq!(engine.get_value(&dk_k3).unwrap().unwrap(), b"v1");
+        assert_eq!(engines.kv.get_value(&dk_k1).unwrap().unwrap(), b"v1");
+        assert_eq!(engines.kv.get_value(&dk_k2).unwrap().unwrap(), b"v1");
+        assert_eq!(engines.kv.get_value(&dk_k3).unwrap().unwrap(), b"v1");
         validate(&router, 1, |delegate| {
             assert_eq!(delegate.applied_index_term, 1);
             assert_eq!(delegate.apply_state.get_applied_index(), 1);
@@ -4886,7 +4952,7 @@ mod tests {
         assert_eq!(apply_res.metrics.size_diff_hint, 5);
         assert_eq!(apply_res.metrics.lock_cf_written_bytes, 5);
         assert_eq!(
-            engine.get_value_cf(CF_LOCK, &dk_k1).unwrap().unwrap(),
+            engines.kv.get_value_cf(CF_LOCK, &dk_k1).unwrap().unwrap(),
             b"v1"
         );
 
@@ -4931,7 +4997,7 @@ mod tests {
         assert_eq!(apply_res.applied_index_term, 2);
         assert_eq!(apply_res.apply_state.get_applied_index(), 4);
         // a writebatch should be atomic.
-        assert_eq!(engine.get_value(&dk_k3).unwrap().unwrap(), b"v1");
+        assert_eq!(engines.kv.get_value(&dk_k3).unwrap().unwrap(), b"v1");
 
         let put_entry = EntryBuilder::new(5, 3)
             .delete(b"k1")
@@ -4954,7 +5020,7 @@ mod tests {
         assert!(resp.get_header().get_error().has_stale_command());
         let resp = capture_rx.recv_timeout(Duration::from_secs(3)).unwrap();
         assert!(!resp.get_header().has_error(), "{:?}", resp);
-        assert!(engine.get_value(&dk_k1).unwrap().is_none());
+        assert!(engines.kv.get_value(&dk_k1).unwrap().is_none());
         let apply_res = fetch_apply_res(&rx);
         assert_eq!(apply_res.metrics.lock_cf_written_bytes, 3);
         assert_eq!(apply_res.metrics.delete_keys_hint, 2);
@@ -4991,7 +5057,7 @@ mod tests {
         );
         let resp = capture_rx.recv_timeout(Duration::from_secs(3)).unwrap();
         assert!(resp.get_header().get_error().has_key_not_in_region());
-        assert_eq!(engine.get_value(&dk_k3).unwrap().unwrap(), b"v1");
+        assert_eq!(engines.kv.get_value(&dk_k3).unwrap().unwrap(), b"v1");
         fetch_apply_res(&rx);
 
         let delete_range_entry = EntryBuilder::new(8, 3)
@@ -5012,9 +5078,9 @@ mod tests {
         );
         let resp = capture_rx.recv_timeout(Duration::from_secs(3)).unwrap();
         assert!(!resp.get_header().has_error(), "{:?}", resp);
-        assert!(engine.get_value(&dk_k1).unwrap().is_none());
-        assert!(engine.get_value(&dk_k2).unwrap().is_none());
-        assert!(engine.get_value(&dk_k3).unwrap().is_none());
+        assert!(engines.kv.get_value(&dk_k1).unwrap().is_none());
+        assert!(engines.kv.get_value(&dk_k2).unwrap().is_none());
+        assert!(engines.kv.get_value(&dk_k3).unwrap().is_none());
 
         // The region was rescheduled from normal-priority handler to
         // low-priority handler, so the first apple_res.exec_res should be empty.
@@ -5088,7 +5154,7 @@ mod tests {
         assert!(!resp.get_header().has_error(), "{:?}", resp);
         let resp = capture_rx.recv_timeout(Duration::from_secs(3)).unwrap();
         assert!(!resp.get_header().has_error(), "{:?}", resp);
-        check_db_range(&engine, sst_range);
+        check_db_range(&engines.kv, sst_range);
         let resp = capture_rx.recv_timeout(Duration::from_secs(3)).unwrap();
         assert!(resp.get_header().has_error());
 
@@ -5139,7 +5205,7 @@ mod tests {
 
     #[test]
     fn test_handle_ingest_sst() {
-        let (_path, engine) = create_tmp_engine("test-ingest");
+        let (_path, engines) = create_tmp_engines("test-ingest");
         let (import_dir, importer) = create_tmp_importer("test-ingest");
         let obs = ApplyObserver::default();
         let mut host = CoprocessorHost::<KvTestEngine>::default();
@@ -5157,14 +5223,14 @@ mod tests {
         };
         let (router, mut system) = create_apply_batch_system(&cfg.value());
         let pending_create_peers = Arc::new(Mutex::new(HashMap::default()));
-        let builder = super::Builder::<KvTestEngine, KvTestWriteBatch> {
+        let builder = super::Builder::<KvTestEngine, RaftTestEngine> {
             tag: "test-store".to_owned(),
             cfg,
             sender,
             region_scheduler,
             coprocessor_host: host,
             importer: importer.clone(),
-            engine: engine.clone(),
+            engines: engines.clone(),
             router: router.clone(),
             _phantom: Default::default(),
             store_id: 1,
@@ -5313,13 +5379,13 @@ mod tests {
         // Verify the engine keys.
         for i in 1..keys_count {
             let dk = keys::data_key(&keys[i]);
-            assert_eq!(engine.get_value(&dk).unwrap().unwrap(), &expected_vals[i]);
+            assert_eq!(engines.kv.get_value(&dk).unwrap().unwrap(), &expected_vals[i]);
         }
     }
 
     #[test]
     fn test_cmd_observer() {
-        let (_path, engine) = create_tmp_engine("test-delegate");
+        let (_path, engines) = create_tmp_engines("test-delegate");
         let (_import_dir, importer) = create_tmp_importer("test-delegate");
         let mut host = CoprocessorHost::<KvTestEngine>::default();
         let mut obs = ApplyObserver::default();
@@ -5334,14 +5400,14 @@ mod tests {
         let cfg = Config::default();
         let (router, mut system) = create_apply_batch_system(&cfg);
         let pending_create_peers = Arc::new(Mutex::new(HashMap::default()));
-        let builder = super::Builder::<KvTestEngine, KvTestWriteBatch> {
+        let builder = super::Builder::<KvTestEngine, RaftTestEngine> {
             tag: "test-store".to_owned(),
             cfg: Arc::new(VersionTrack::new(cfg)),
             sender,
             region_scheduler,
             coprocessor_host: host,
             importer,
-            engine,
+            engines,
             router: router.clone(),
             _phantom: Default::default(),
             store_id: 1,
@@ -5597,7 +5663,7 @@ mod tests {
 
     #[test]
     fn test_split() {
-        let (_path, engine) = create_tmp_engine("test-delegate");
+        let (_path, engines) = create_tmp_engines("test-delegate");
         let (_import_dir, importer) = create_tmp_importer("test-delegate");
         let peer_id = 3;
         let mut reg = Registration {
@@ -5623,14 +5689,14 @@ mod tests {
         let cfg = Arc::new(VersionTrack::new(Config::default()));
         let (router, mut system) = create_apply_batch_system(&cfg.value());
         let pending_create_peers = Arc::new(Mutex::new(HashMap::default()));
-        let builder = super::Builder::<KvTestEngine, KvTestWriteBatch> {
+        let builder = super::Builder::<KvTestEngine, RaftTestEngine> {
             tag: "test-store".to_owned(),
             cfg,
             sender,
             importer,
             region_scheduler,
             coprocessor_host: host,
-            engine: engine.clone(),
+            engines: engines.clone(),
             router: router.clone(),
             _phantom: Default::default(),
             store_id: 2,
@@ -5660,7 +5726,7 @@ mod tests {
         let (capture_tx, capture_rx) = mpsc::channel();
         let epoch = Rc::new(RefCell::new(reg.region.get_region_epoch().to_owned()));
         let epoch_ = epoch.clone();
-        let mut exec_split = |router: &ApplyRouter<KvTestEngine>, reqs| {
+        let mut exec_split = |router: &ApplyRouter<KvTestEngine, RaftTestEngine>, reqs| {
             let epoch = epoch_.borrow();
             let split = EntryBuilder::new(index_id, 1)
                 .split(reqs)
@@ -5733,7 +5799,7 @@ mod tests {
         // All requests should be checked.
         assert!(error_msg(&resp).contains("id count"), "{:?}", resp);
         let checker = SplitResultChecker {
-            engine,
+            engine: engines.kv,
             origin_peers: &peers,
             epoch: epoch.clone(),
         };
diff --git a/components/raftstore/src/store/fsm/peer.rs b/components/raftstore/src/store/fsm/peer.rs
index 8991628b0..02c097c54 100644
--- a/components/raftstore/src/store/fsm/peer.rs
+++ b/components/raftstore/src/store/fsm/peer.rs
@@ -755,6 +755,7 @@ where
             }
         };
         let mut kv_wb = self.ctx.engines.kv.write_batch();
+        let mut r_wb = self.ctx.engines.raft.log_batch(0);
         write_peer_state(&mut kv_wb, &region, PeerState::Normal, None).unwrap_or_else(|e| {
             panic!(
                 "fails to write RegionLocalState {:?} into write brach, err {:?}",
@@ -762,11 +763,24 @@ where
             )
         });
         let mut write_opts = WriteOptions::new();
-        write_opts.set_sync(true);
+        write_opts.set_sync(false);
+        write_opts.set_disable_wal(true);
+        // write to kv engine        
         if let Err(e) = kv_wb.write_opt(&write_opts) {
             panic!("fail to update RegionLocalstate {:?} err {:?}", region, e);
         }
 
+        // write to raft engine
+        if let Err(e) = self.ctx.engines.raft.consume(&mut r_wb, true) {
+            panic!("fail to update RegionLocalstate {:?} err {:?}", region, e);
+        }
+            
+        // let mut write_opts = WriteOptions::new();
+        // write_opts.set_sync(true);
+        // if let Err(e) = kv_wb.write_opt(&write_opts) {
+        //     panic!("fail to update RegionLocalstate {:?} err {:?}", region, e);
+        // }
+
         {
             let mut meta = self.ctx.store_meta.lock().unwrap();
             meta.set_region(
diff --git a/components/raftstore/src/store/fsm/store.rs b/components/raftstore/src/store/fsm/store.rs
index 4a3e015dc..344e9cf48 100644
--- a/components/raftstore/src/store/fsm/store.rs
+++ b/components/raftstore/src/store/fsm/store.rs
@@ -367,7 +367,7 @@ where
     pub cleanup_scheduler: Scheduler<CleanupTask>,
     pub raftlog_gc_scheduler: Scheduler<RaftlogGcTask>,
     pub region_scheduler: Scheduler<RegionTask<EK::Snapshot>>,
-    pub apply_router: ApplyRouter<EK>,
+    pub apply_router: ApplyRouter<EK, ER>,
     pub router: RaftRouter<EK, ER>,
     pub importer: Arc<SSTImporter>,
     pub store_meta: Arc<Mutex<StoreMeta>>,
@@ -939,7 +939,7 @@ pub struct RaftPollerBuilder<EK: KvEngine, ER: RaftEngine, T> {
     cleanup_scheduler: Scheduler<CleanupTask>,
     raftlog_gc_scheduler: Scheduler<RaftlogGcTask>,
     pub region_scheduler: Scheduler<RegionTask<EK::Snapshot>>,
-    apply_router: ApplyRouter<EK>,
+    apply_router: ApplyRouter<EK, ER>,
     pub router: RaftRouter<EK, ER>,
     pub importer: Arc<SSTImporter>,
     pub store_meta: Arc<Mutex<StoreMeta>>,
@@ -1220,8 +1220,8 @@ struct Workers<EK: KvEngine, ER: RaftEngine> {
 
 pub struct RaftBatchSystem<EK: KvEngine, ER: RaftEngine> {
     system: BatchSystem<PeerFsm<EK, ER>, StoreFsm<EK>>,
-    apply_router: ApplyRouter<EK>,
-    apply_system: ApplyBatchSystem<EK>,
+    apply_router: ApplyRouter<EK, ER>,
+    apply_system: ApplyBatchSystem<EK, ER>,
     router: RaftRouter<EK, ER>,
     workers: Option<Workers<EK, ER>>,
     store_writers: StoreWriters<EK, ER>,
@@ -1232,7 +1232,7 @@ impl<EK: KvEngine, ER: RaftEngine> RaftBatchSystem<EK, ER> {
         self.router.clone()
     }
 
-    pub fn apply_router(&self) -> ApplyRouter<EK> {
+    pub fn apply_router(&self) -> ApplyRouter<EK, ER> {
         self.apply_router.clone()
     }
 
@@ -1378,7 +1378,7 @@ impl<EK: KvEngine, ER: RaftEngine> RaftBatchSystem<EK, ER> {
         let cfg = builder.cfg.value().clone();
         let store = builder.store.clone();
 
-        let apply_poller_builder = ApplyPollerBuilder::<EK, W>::new(
+        let apply_poller_builder = ApplyPollerBuilder::<EK, ER>::new(
             &builder,
             Box::new(self.router.clone()),
             self.apply_router.clone(),
@@ -2524,11 +2524,13 @@ impl<'a, EK: KvEngine, ER: RaftEngine, T: Transport> StoreFsmDelegate<'a, EK, ER
     fn on_create_peer(&self, region: Region) {
         info!("creating a peer"; "peer" => ?region);
         let mut kv_wb = self.ctx.engines.kv.write_batch();
+        let mut raft_wb = self.ctx.engines.raft.log_batch(0);
         let region_state_key = keys::region_state_key(region.get_id());
         match self
             .ctx
             .engines
             .kv
+        // .get_raft_region_state(region.get_id()) use if reading from raft engine
             .get_msg_cf::<RegionLocalState>(CF_RAFT, &region_state_key)
         {
             Ok(Some(region_state)) => {
@@ -2550,11 +2552,22 @@ impl<'a, EK: KvEngine, ER: RaftEngine, T: Transport> StoreFsmDelegate<'a, EK, ER
                     region, e
                 )
             });
+        // write to kv engine
         let mut write_opts = WriteOptions::new();
-        write_opts.set_sync(true);
+        write_opts.set_sync(false);
+        write_opts.set_disable_wal(true);
         if let Err(e) = kv_wb.write_opt(&write_opts) {
-            panic!("fail to write while creating {:?} err {:?}", region, e);
-        }
+            panic!("fail to update RegionLocalstate {:?} err {:?}", region, e);
+        }        
+        // write to raft engine
+        if let Err(e) = self.ctx.engines.raft.consume(&mut raft_wb, true) {
+            panic!("fail to update RegionLocalstate {:?} err {:?}", region, e);
+        }
+        // let mut write_opts = WriteOptions::new();
+        // write_opts.set_sync(true);
+        // if let Err(e) = kv_wb.write_opt(&write_opts) {
+        //     panic!("fail to write while creating {:?} err {:?}", region, e);
+        // }
         let (sender, mut peer) = match PeerFsm::create(
             self.ctx.store.get_id(),
             &self.ctx.cfg,
diff --git a/components/raftstore/src/store/peer_storage.rs b/components/raftstore/src/store/peer_storage.rs
index 06bbeb96e..389e91265 100644
--- a/components/raftstore/src/store/peer_storage.rs
+++ b/components/raftstore/src/store/peer_storage.rs
@@ -512,6 +512,8 @@ fn init_apply_state<EK: KvEngine, ER: RaftEngine>(
 ) -> Result<RaftApplyState> {
     Ok(
         match engines
+//            .raft
+        //            .get_raft_apply_state(region.get_id())?
             .kv
             .get_msg_cf(CF_RAFT, &keys::apply_state_key(region.get_id()))?
         {
@@ -1718,7 +1720,10 @@ pub fn write_initial_raft_state<W: RaftLogBatch>(raft_wb: &mut W, region_id: u64
 
 // When we bootstrap the region or handling split new region, we must
 // call this to initialize region apply state first.
-pub fn write_initial_apply_state<T: Mutable>(kv_wb: &mut T, region_id: u64) -> Result<()> {
+pub fn write_initial_apply_state<T: Mutable>(
+    kv_wb: &mut T,
+    region_id: u64
+) -> Result<()> {
     let mut apply_state = RaftApplyState::default();
     apply_state.set_applied_index(RAFT_INIT_LOG_INDEX);
     apply_state
@@ -1732,8 +1737,8 @@ pub fn write_initial_apply_state<T: Mutable>(kv_wb: &mut T, region_id: u64) -> R
     Ok(())
 }
 
-pub fn write_peer_state<T: Mutable>(
-    kv_wb: &mut T,
+pub fn write_peer_state<Q: Mutable>(
+    kv_wb: &mut Q,
     region: &metapb::Region,
     state: PeerState,
     merge_state: Option<MergeState>,
diff --git a/components/raftstore/src/store/worker/raftlog_gc.rs b/components/raftstore/src/store/worker/raftlog_gc.rs
index dc46cf298..42b0fe14f 100644
--- a/components/raftstore/src/store/worker/raftlog_gc.rs
+++ b/components/raftstore/src/store/worker/raftlog_gc.rs
@@ -123,9 +123,13 @@ impl<EK: KvEngine, ER: RaftEngine, R: CasualRouter<EK>> Runner<EK, ER, R> {
         }
         // Sync wal of kv_db to make sure the data before apply_index has been persisted to disk.
         let start = Instant::now();
-        self.engines.kv.sync().unwrap_or_else(|e| {
-            panic!("failed to sync kv_engine in raft_log_gc: {:?}", e);
+        self.engines.kv.flush_all().unwrap_or_else(|e| {
+            panic!("failed to flush kv_engine in raft_log_gc: {:?}", e);
         });
+
+        // self.engines.kv.sync().unwrap_or_else(|e| {
+        //     panic!("failed to sync kv_engine in raft_log_gc: {:?}", e);
+        // });
         RAFT_LOG_GC_KV_SYNC_DURATION_HISTOGRAM.observe(start.saturating_elapsed_secs());
         let tasks = std::mem::take(&mut self.tasks);
         let mut groups = Vec::with_capacity(tasks.len());
diff --git a/components/server/src/setup.rs b/components/server/src/setup.rs
index 547f1cd8d..e2140c5c4 100644
--- a/components/server/src/setup.rs
+++ b/components/server/src/setup.rs
@@ -210,6 +210,9 @@ pub fn initial_metric(cfg: &MetricConfig) {
 
 #[allow(dead_code)]
 pub fn overwrite_config_with_cmd_args(config: &mut TiKvConfig, matches: &ArgMatches<'_>) {
+    if matches.is_present("fail-on-write") {
+         config.rocksdb.fail_on_write = true;
+    }
     if let Some(level) = matches.value_of("log-level") {
         config.log_level = logger::get_level_by_string(level).unwrap();
     }
diff --git a/components/test_raftstore/src/cluster.rs b/components/test_raftstore/src/cluster.rs
index 60d53dcfe..bc3f1e187 100644
--- a/components/test_raftstore/src/cluster.rs
+++ b/components/test_raftstore/src/cluster.rs
@@ -5,6 +5,7 @@ use std::error::Error as StdError;
 use std::sync::{mpsc, Arc, Mutex, RwLock};
 use std::time::Duration;
 use std::{result, thread};
+use std::convert::TryInto;
 
 use crossbeam::channel::TrySendError;
 use futures::executor::block_on;
@@ -22,7 +23,7 @@ use tempfile::TempDir;
 use crate::Config;
 use collections::{HashMap, HashSet};
 use encryption_export::DataKeyManager;
-use engine_rocks::raw::DB;
+use engine_rocks::raw::{DB, Env};
 use engine_rocks::{Compat, RocksEngine, RocksSnapshot};
 use engine_traits::{
     CompactExt, Engines, Iterable, MiscExt, Mutable, Peekable, WriteBatch, WriteBatchExt,
@@ -94,7 +95,7 @@ pub trait Simulator {
         let node_id = request.get_header().get_peer().get_store_id();
         self.call_command_on_node(node_id, request, timeout)
     }
-
+    
     fn read(
         &self,
         batch_id: Option<ThreadReadId>,
@@ -144,6 +145,7 @@ pub struct Cluster<T: Simulator> {
 
     pub paths: Vec<TempDir>,
     pub dbs: Vec<Engines<RocksEngine, RocksEngine>>,
+    pub kv_envs: Vec<Arc<Env>>,
     pub store_metas: HashMap<u64, Arc<Mutex<StoreMeta>>>,
     key_managers: Vec<Option<Arc<DataKeyManager>>>,
     pub io_rate_limiter: Option<Arc<IORateLimiter>>,
@@ -154,6 +156,7 @@ pub struct Cluster<T: Simulator> {
 
     pub sim: Arc<RwLock<T>>,
     pub pd_client: Arc<TestPdClient>,
+    pub fault: bool,
 }
 
 impl<T: Simulator> Cluster<T> {
@@ -167,13 +170,44 @@ impl<T: Simulator> Cluster<T> {
         // TODO: In the future, maybe it's better to test both case where `use_delete_range` is true and false
         Cluster {
             cfg: Config {
-                tikv: new_tikv_config(id),
+                tikv: new_tikv_config(id, false),
+                prefer_mem: true,
+            },
+            leaders: HashMap::default(),
+            count,
+            paths: vec![],
+            dbs: vec![],
+            kv_envs: vec![],
+            store_metas: HashMap::default(),
+            key_managers: vec![],
+            io_rate_limiter: None,
+            engines: HashMap::default(),
+            key_managers_map: HashMap::default(),
+            labels: HashMap::default(),
+            group_props: HashMap::default(),
+            sim,
+            pd_client,
+            fault: false,
+        }
+    }
+
+    // shawgerj: same as Cluster::new() but use special fault environment
+    pub fn new_fault(
+        id: u64,
+        count: usize,
+        sim: Arc<RwLock<T>>,
+        pd_client: Arc<TestPdClient>,
+    ) -> Cluster<T> {
+        Cluster {
+            cfg: Config {
+                tikv: new_tikv_config(id, true),
                 prefer_mem: true,
             },
             leaders: HashMap::default(),
             count,
             paths: vec![],
             dbs: vec![],
+            kv_envs: vec![],
             store_metas: HashMap::default(),
             key_managers: vec![],
             io_rate_limiter: None,
@@ -183,6 +217,7 @@ impl<T: Simulator> Cluster<T> {
             group_props: HashMap::default(),
             sim,
             pd_client,
+            fault: true,
         }
     }
 
@@ -216,11 +251,12 @@ impl<T: Simulator> Cluster<T> {
     }
 
     fn create_engine(&mut self, router: Option<RaftRouter<RocksEngine, RocksEngine>>) {
-        let (engines, key_manager, dir) =
-            create_test_engine(router, self.io_rate_limiter.clone(), &self.cfg);
+        let (engines, key_manager, dir, kv_env) =
+            create_test_engine(router, self.io_rate_limiter.clone(), &self.cfg, self.fault);
         self.dbs.push(engines);
         self.key_managers.push(key_manager);
         self.paths.push(dir);
+        self.kv_envs.push(kv_env);
     }
 
     pub fn create_engines(&mut self) {
@@ -235,6 +271,36 @@ impl<T: Simulator> Cluster<T> {
         }
     }
 
+    pub fn reboot_engine(&mut self, engine_id: u64) {
+        let i: usize = (engine_id - 1).try_into().unwrap();
+        // remove engine from dbs vec, engines hashmap, and key manager
+        {
+            let engines = self.engines.remove(&engine_id);
+            self.dbs.remove(i);
+            self.kv_envs.remove(i);
+            self.key_managers.remove(i);
+            if let Some(engines) = engines {
+                // TODO: figure out why there is a race here... 
+                sleep_ms(5000);
+                
+                drop(engines.kv);
+                drop(engines.raft);
+            }
+        }
+
+        let (engines, key_manager, kv_env) =
+            create_test_engine_with_path(&self.paths[i],
+                                         self.io_rate_limiter.clone(),
+                                         &self.cfg,
+                                         self.fault);
+
+        // insert the engine back into dbs vec and engines hashmap
+        self.dbs.insert(i, engines);
+        self.kv_envs.insert(i, kv_env);
+        self.key_managers.insert(i, key_manager);
+        self.engines.insert(engine_id, self.dbs[i].clone());
+    }
+
     pub fn start(&mut self) -> ServerResult<()> {
         // Try recover from last shutdown.
         let node_ids: Vec<u64> = self.engines.iter().map(|(&id, _)| id).collect();
@@ -268,7 +334,7 @@ impl<T: Simulator> Cluster<T> {
             self.engines.insert(node_id, engines);
             self.store_metas.insert(node_id, store_meta);
             self.key_managers_map.insert(node_id, key_mgr);
-        }
+        } 
         Ok(())
     }
 
@@ -1130,8 +1196,18 @@ impl<T: Simulator> Cluster<T> {
             .unwrap()
     }
 
+    pub fn raft_commit_index(&self, region_id: u64, store_id: u64) -> u64 {
+        let key = keys::raft_state_key(region_id);
+        self.get_raft_engine(store_id)
+            .c()
+            .get_msg::<raft_serverpb::RaftLocalState>(&key)
+            .unwrap()
+            .unwrap()
+            .get_hard_state().get_commit()
+    }
+
     pub fn region_local_state(&self, region_id: u64, store_id: u64) -> RegionLocalState {
-        self.get_engine(store_id)
+        self.get_raft_engine(store_id)
             .c()
             .get_msg_cf::<RegionLocalState>(
                 engine_traits::CF_RAFT,
@@ -1144,7 +1220,7 @@ impl<T: Simulator> Cluster<T> {
     pub fn must_peer_state(&self, region_id: u64, store_id: u64, peer_state: PeerState) {
         for _ in 0..100 {
             let state = self
-                .get_engine(store_id)
+                .get_raft_engine(store_id)
                 .c()
                 .get_msg_cf::<RegionLocalState>(
                     engine_traits::CF_RAFT,
diff --git a/components/test_raftstore/src/common-test.toml b/components/test_raftstore/src/common-test.toml
index 0442d4f51..363efee49 100644
--- a/components/test_raftstore/src/common-test.toml
+++ b/components/test_raftstore/src/common-test.toml
@@ -97,6 +97,8 @@ max-background-gc = 1
 
 [raftdb.defaultcf]
 
+[raftdb.raftcf]
+
 [raftdb.defaultcf.titan]
 
 [security]
diff --git a/components/test_raftstore/src/node.rs b/components/test_raftstore/src/node.rs
index 3a73f7aee..1eaff1044 100644
--- a/components/test_raftstore/src/node.rs
+++ b/components/test_raftstore/src/node.rs
@@ -353,6 +353,7 @@ impl Simulator for NodeCluster {
         if let Some(mut node) = self.nodes.remove(&node_id) {
             node.stop();
         }
+        self.simulate_trans.remove(&node_id).unwrap();
         self.trans
             .core
             .lock()
@@ -360,6 +361,13 @@ impl Simulator for NodeCluster {
             .routers
             .remove(&node_id)
             .unwrap();
+        self.trans
+            .core
+            .lock()
+            .unwrap()
+            .snap_paths
+            .remove(&node_id)
+            .unwrap();
     }
 
     fn get_node_ids(&self) -> HashSet<u64> {
@@ -461,6 +469,12 @@ pub fn new_node_cluster(id: u64, count: usize) -> Cluster<NodeCluster> {
     Cluster::new(id, count, sim, pd_client)
 }
 
+pub fn new_fault_node_cluster(id: u64, count: usize) -> Cluster<NodeCluster> {
+    let pd_client = Arc::new(TestPdClient::new(id, false));
+    let sim = Arc::new(RwLock::new(NodeCluster::new(Arc::clone(&pd_client))));
+    Cluster::new_fault(id, count, sim, pd_client)
+}
+
 pub fn new_incompatible_node_cluster(id: u64, count: usize) -> Cluster<NodeCluster> {
     let pd_client = Arc::new(TestPdClient::new(id, true));
     let sim = Arc::new(RwLock::new(NodeCluster::new(Arc::clone(&pd_client))));
diff --git a/components/test_raftstore/src/server.rs b/components/test_raftstore/src/server.rs
index 9c32a3473..8eaf7362b 100644
--- a/components/test_raftstore/src/server.rs
+++ b/components/test_raftstore/src/server.rs
@@ -113,7 +113,7 @@ struct ServerMeta {
     sim_router: SimulateStoreTransport,
     sim_trans: SimulateServerTransport,
     raw_router: RaftRouter<RocksEngine, RocksEngine>,
-    raw_apply_router: ApplyRouter<RocksEngine>,
+    raw_apply_router: ApplyRouter<RocksEngine, RocksEngine>,
     gc_worker: GcWorker<RaftKv<RocksEngine, SimulateStoreTransport>, SimulateStoreTransport>,
     rts_worker: Option<LazyWorker<resolved_ts::Task<RocksSnapshot>>>,
 }
@@ -186,7 +186,7 @@ impl ServerCluster {
         self.addrs.get(node_id).unwrap()
     }
 
-    pub fn get_apply_router(&self, node_id: u64) -> ApplyRouter<RocksEngine> {
+    pub fn get_apply_router(&self, node_id: u64) -> ApplyRouter<RocksEngine, RocksEngine> {
         self.metas.get(&node_id).unwrap().raw_apply_router.clone()
     }
 
diff --git a/components/test_raftstore/src/util.rs b/components/test_raftstore/src/util.rs
index 73f153ee5..eb83c7ca7 100644
--- a/components/test_raftstore/src/util.rs
+++ b/components/test_raftstore/src/util.rs
@@ -11,11 +11,11 @@ use encryption_export::{
     data_key_manager_from_config, DataKeyManager, FileConfig, MasterKeyConfig,
 };
 use engine_rocks::config::BlobRunMode;
-use engine_rocks::raw::DB;
+use engine_rocks::raw::{DB, Env};
 use engine_rocks::{
-    get_env, CompactionListener, Compat, RocksCompactionJobInfo, RocksEngine, RocksSnapshot,
+    get_env, get_fault_injection_env, CompactionListener, Compat, RocksCompactionJobInfo, RocksEngine, RocksSnapshot,
 };
-use engine_traits::{Engines, Iterable, Peekable, ALL_CFS, CF_DEFAULT, CF_RAFT};
+use engine_traits::{Engines, Iterable, Peekable, ALL_CFS, CF_DEFAULT, CF_RAFT, SyncMutable};
 use file_system::IORateLimiter;
 use futures::executor::block_on;
 use grpcio::{ChannelBuilder, Environment};
@@ -50,6 +50,10 @@ use crate::{Cluster, ServerCluster, Simulator, TestPdClient};
 
 pub use raftstore::store::util::{find_peer, new_learner_peer, new_peer};
 
+pub fn must_delete(engine: &Arc<DB>, key: &[u8]) {
+    engine.c().delete(&keys::data_key(key)).unwrap();
+}
+
 pub fn must_get(engine: &Arc<DB>, cf: &str, key: &[u8], value: Option<&[u8]>) {
     for _ in 1..300 {
         let res = engine.c().get_value_cf(cf, &keys::data_key(key)).unwrap();
@@ -136,9 +140,13 @@ lazy_static! {
     };
 }
 
-pub fn new_tikv_config(cluster_id: u64) -> TiKvConfig {
+pub fn new_tikv_config(cluster_id: u64, fault: bool) -> TiKvConfig {
     let mut cfg = TEST_CONFIG.clone();
     cfg.server.cluster_id = cluster_id;
+    if fault {
+        cfg.rocksdb.fail_on_write = true;
+        cfg.rocksdb.manual_wal_flush = true;
+    }  
     cfg
 }
 
@@ -614,10 +622,12 @@ pub fn create_test_engine(
     router: Option<RaftRouter<RocksEngine, RocksEngine>>,
     limiter: Option<Arc<IORateLimiter>>,
     cfg: &Config,
+    fault: bool,
 ) -> (
     Engines<RocksEngine, RocksEngine>,
     Option<Arc<DataKeyManager>>,
     TempDir,
+    Arc<Env>,
 ) {
     let dir = test_util::temp_dir("test_cluster", cfg.prefer_mem);
     let key_manager =
@@ -625,13 +635,18 @@ pub fn create_test_engine(
             .unwrap()
             .map(Arc::new);
 
-    let env = get_env(key_manager.clone(), limiter).unwrap();
     let cache = cfg.storage.block_cache.build_shared_cache();
 
     let kv_path = dir.path().join(DEFAULT_ROCKSDB_SUB_DIR);
     let kv_path_str = kv_path.to_str().unwrap();
-
     let mut kv_db_opt = cfg.rocksdb.build_opt();
+    
+    let env:Arc<Env>;    
+    if fault {
+        env = get_fault_injection_env().unwrap();
+    } else {
+        env = get_env(key_manager.clone(), limiter.clone()).unwrap();
+    }
     kv_db_opt.set_env(env.clone());
 
     if let Some(router) = router {
@@ -661,7 +676,68 @@ pub fn create_test_engine(
     let raft_path_str = raft_path.to_str().unwrap();
 
     let mut raft_db_opt = cfg.raftdb.build_opt();
-    raft_db_opt.set_env(env);
+    let raft_env = get_env(key_manager.clone(), limiter).unwrap();
+    raft_db_opt.set_env(raft_env);
+
+    let raft_cfs_opt = cfg.raftdb.build_cf_opts(&cache);
+    let raft_engine = Arc::new(
+        engine_rocks::raw_util::new_engine_opt(raft_path_str, raft_db_opt, raft_cfs_opt).unwrap(),
+    );
+
+    let mut engine = RocksEngine::from_db(engine);
+    let mut raft_engine = RocksEngine::from_db(raft_engine);
+    let shared_block_cache = cache.is_some();
+    engine.set_shared_block_cache(shared_block_cache);
+    raft_engine.set_shared_block_cache(shared_block_cache);
+    let engines = Engines::new(engine, raft_engine);
+    (engines, key_manager, dir, env)
+}
+
+pub fn create_test_engine_with_path(
+    // TODO: pass it in for all cases.
+    dir: &TempDir,
+    limiter: Option<Arc<IORateLimiter>>,
+    cfg: &Config,
+    fault: bool,
+) -> (
+    Engines<RocksEngine, RocksEngine>,
+    Option<Arc<DataKeyManager>>,
+    Arc<Env>,
+) {
+    let key_manager =
+        data_key_manager_from_config(&cfg.security.encryption, dir.path().to_str().unwrap())
+            .unwrap()
+            .map(Arc::new);
+
+    let cache = cfg.storage.block_cache.build_shared_cache();
+
+    let kv_path = dir.path().join(DEFAULT_ROCKSDB_SUB_DIR);
+    let kv_path_str = kv_path.to_str().unwrap();
+
+    let mut kv_db_opt = cfg.rocksdb.build_opt();
+    
+    let env:Arc<Env>;    
+    if fault {
+        env = get_fault_injection_env().unwrap();
+    } else {
+        env = get_env(key_manager.clone(), limiter.clone()).unwrap();
+    }
+    kv_db_opt.set_env(env.clone());
+
+    let kv_cfs_opt = cfg
+        .rocksdb
+        .build_cf_opts(&cache, None, cfg.storage.enable_ttl);
+
+    let engine = Arc::new(
+        engine_rocks::raw_util::new_engine_opt(kv_path_str, kv_db_opt, kv_cfs_opt).unwrap(),
+    );
+
+    let raft_path = dir.path().join("raft");
+    let raft_path_str = raft_path.to_str().unwrap();
+
+    let mut raft_db_opt = cfg.raftdb.build_opt();
+    let raft_env = get_env(key_manager.clone(), limiter).unwrap();
+    raft_db_opt.set_env(raft_env);
 
     let raft_cfs_opt = cfg.raftdb.build_cf_opts(&cache);
     let raft_engine = Arc::new(
@@ -674,9 +750,10 @@ pub fn create_test_engine(
     engine.set_shared_block_cache(shared_block_cache);
     raft_engine.set_shared_block_cache(shared_block_cache);
     let engines = Engines::new(engine, raft_engine);
-    (engines, key_manager, dir)
+    (engines, key_manager, env)
 }
 
+
 pub fn configure_for_request_snapshot<T: Simulator>(cluster: &mut Cluster<T>) {
     // We don't want to generate snapshots due to compact log.
     cluster.cfg.raft_store.raft_log_gc_threshold = 1000;
diff --git a/src/config.rs b/src/config.rs
index a16db42d8..0436cc0ab 100644
--- a/src/config.rs
+++ b/src/config.rs
@@ -897,6 +897,10 @@ impl TitanDBConfig {
 #[serde(default)]
 #[serde(rename_all = "kebab-case")]
 pub struct DbConfig {
+    #[online_config(skip)]
+    pub manual_wal_flush: bool,
+    #[online_config(skip)]
+    pub fail_on_write: bool,
     #[online_config(skip)]
     pub info_log_level: LogLevel,
     #[serde(with = "rocks_config::recovery_mode_serde")]
@@ -974,6 +978,8 @@ impl Default for DbConfig {
             ..Default::default()
         };
         DbConfig {
+            manual_wal_flush: false,
+            fail_on_write: false,
             wal_recovery_mode: DBRecoveryMode::PointInTime,
             wal_dir: "".to_owned(),
             wal_ttl_seconds: 0,
@@ -1017,6 +1023,10 @@ impl Default for DbConfig {
 impl DbConfig {
     pub fn build_opt(&self) -> DBOptions {
         let mut opts = DBOptions::new();
+        // shawgerj: necessary for atomic flush of multiple column families
+        // with WAL disabled. https://github.com/facebook/rocksdb/wiki/Atomic-flush
+        opts.set_atomic_flush(true);
+        opts.set_fail_on_write(self.fail_on_write);
         opts.set_wal_recovery_mode(self.wal_recovery_mode);
         if !self.wal_dir.is_empty() {
             opts.set_wal_dir(&self.wal_dir);
@@ -1252,6 +1262,8 @@ pub struct RaftDbConfig {
     pub wal_bytes_per_sync: ReadableSize,
     #[online_config(submodule)]
     pub defaultcf: RaftDefaultCfConfig,
+    #[online_config(submodule)]
+    pub raftcf: RaftCfConfig,
     #[online_config(skip)]
     pub titan: TitanDBConfig,
 }
@@ -1291,6 +1303,7 @@ impl Default for RaftDbConfig {
             bytes_per_sync: ReadableSize::mb(1),
             wal_bytes_per_sync: ReadableSize::kb(512),
             defaultcf: RaftDefaultCfConfig::default(),
+            raftcf: RaftCfConfig::default(),
             titan: titan_config,
         }
     }
@@ -1340,7 +1353,8 @@ impl RaftDbConfig {
     }
 
     pub fn build_cf_opts(&self, cache: &Option<Cache>) -> Vec<CFOptions<'_>> {
-        vec![CFOptions::new(CF_DEFAULT, self.defaultcf.build_opt(cache))]
+        vec![CFOptions::new(CF_DEFAULT, self.defaultcf.build_opt(cache)),
+             CFOptions::new(CF_RAFT, self.raftcf.build_opt(cache))]
     }
 
     fn validate(&mut self) -> Result<(), Box<dyn Error>> {
@@ -2340,6 +2354,9 @@ pub struct TiKvConfig {
     #[online_config(hidden)]
     pub cfg_path: String,
 
+    #[online_config(skip)]
+    pub fail_on_write: bool,
+
     #[online_config(skip)]
     #[serde(with = "log_level_serde")]
     pub log_level: slog::Level,
@@ -2444,6 +2461,7 @@ pub struct TiKvConfig {
 impl Default for TiKvConfig {
     fn default() -> TiKvConfig {
         TiKvConfig {
+            fail_on_write: false,
             cfg_path: "".to_owned(),
             log_level: slog::Level::Info,
             log_file: "".to_owned(),
diff --git a/src/server/debug.rs b/src/server/debug.rs
index da5f2a347..8001334e6 100644
--- a/src/server/debug.rs
+++ b/src/server/debug.rs
@@ -188,11 +188,10 @@ impl<ER: RaftEngine> Debugger<ER> {
     pub fn region_info(&self, region_id: u64) -> Result<RegionInfo> {
         let raft_state = box_try!(self.engines.raft.get_raft_state(region_id));
 
-        let apply_state_key = keys::apply_state_key(region_id);
         let apply_state = box_try!(
             self.engines
-                .kv
-                .get_msg_cf::<RaftApplyState>(CF_RAFT, &apply_state_key)
+                .raft
+                .get_raft_apply_state(region_id)
         );
 
         let region_state_key = keys::region_state_key(region_id);
@@ -327,32 +326,34 @@ impl<ER: RaftEngine> Debugger<ER> {
     /// peers, version, and key range) from `region` which comes from PD normally.
     pub fn set_region_tombstone(&self, regions: Vec<Region>) -> Result<Vec<(u64, Error)>> {
         let store_id = self.get_store_id()?;
-        let db = &self.engines.kv;
-        let mut wb = db.write_batch();
-
+        let db_kv = &self.engines.kv;
+        let mut wb_kv = db_kv.write_batch();
         let mut errors = Vec::with_capacity(regions.len());
+        
         for region in regions {
             let region_id = region.get_id();
-            if let Err(e) = set_region_tombstone(db.as_inner(), store_id, region, &mut wb) {
+            if let Err(e) = set_region_tombstone(db_kv.as_inner(), store_id, region, &mut wb_kv) {
                 errors.push((region_id, e));
             }
         }
 
         if errors.is_empty() {
             let mut write_opts = WriteOptions::new();
-            write_opts.set_sync(true);
-            box_try!(wb.write_opt(&write_opts));
+            write_opts.set_sync(false);
+            write_opts.set_disable_wal(true);
+            box_try!(wb_kv.write_opt(&write_opts));
         }
         Ok(errors)
     }
 
     pub fn set_region_tombstone_by_id(&self, regions: Vec<u64>) -> Result<Vec<(u64, Error)>> {
-        let db = &self.engines.kv;
-        let mut wb = db.write_batch();
+        let db = &self.engines.raft;
+        let db_kv = &self.engines.kv;
+        let mut wb_kv = db_kv.write_batch();
+        let mut wb = db.log_batch(0);
         let mut errors = Vec::with_capacity(regions.len());
         for region_id in regions {
-            let key = keys::region_state_key(region_id);
-            let region_state = match db.get_msg_cf::<RegionLocalState>(CF_RAFT, &key) {
+            let region_state = match db.get_raft_region_state(region_id) {
                 Ok(Some(state)) => state,
                 Ok(None) => {
                     let error = box_err!("{} region local state not exists", region_id);
@@ -370,12 +371,14 @@ impl<ER: RaftEngine> Debugger<ER> {
                 continue;
             }
             let region = &region_state.get_region();
-            write_peer_state(&mut wb, region, PeerState::Tombstone, None).unwrap();
+            write_peer_state(&mut wb_kv,region, PeerState::Tombstone, None).unwrap();
         }
 
         let mut write_opts = WriteOptions::new();
-        write_opts.set_sync(true);
-        wb.write_opt(&write_opts).unwrap();
+        write_opts.set_sync(false);
+        write_opts.set_disable_wal(true);
+        wb_kv.write_opt(&write_opts).unwrap();
+        box_try!(self.engines.raft.consume(&mut wb, true));
         Ok(errors)
     }
 
@@ -784,6 +787,8 @@ impl<ER: RaftEngine> Debugger<ER> {
         box_try!(kv_wb.put_msg_cf(CF_RAFT, &key, &region_state));
 
         // RaftApplyState.
+        // we'll get this from kv engine. Presume it has been restored from raft engine
+        // on recovery
         let key = keys::apply_state_key(region_id);
         if box_try!(kv.get_msg_cf::<RaftApplyState>(CF_RAFT, &key)).is_some() {
             return Err(Error::Other("Store already has the RaftApplyState".into()));
@@ -797,7 +802,8 @@ impl<ER: RaftEngine> Debugger<ER> {
         box_try!(write_initial_raft_state(&mut raft_wb, region_id));
 
         let mut write_opts = WriteOptions::new();
-        write_opts.set_sync(true);
+        write_opts.set_sync(false);
+        write_opts.set_disable_wal(true);
         box_try!(kv_wb.write_opt(&write_opts));
         box_try!(self.engines.raft.consume(&mut raft_wb, true));
         Ok(())
@@ -1254,6 +1260,7 @@ fn validate_db_and_cf(db: DBType, cf: &str) -> Result<()> {
         | (DBType::Kv, CF_WRITE)
         | (DBType::Kv, CF_LOCK)
         | (DBType::Kv, CF_RAFT)
+        | (DBType::Raft, CF_RAFT)    
         | (DBType::Raft, CF_DEFAULT) => Ok(()),
         _ => Err(Error::InvalidArgument(format!(
             "invalid cf {:?} for db {:?}",
@@ -1263,15 +1270,15 @@ fn validate_db_and_cf(db: DBType, cf: &str) -> Result<()> {
 }
 
 fn set_region_tombstone(
-    db: &Arc<DB>,
+    db_kv: &Arc<DB>,
     store_id: u64,
     region: Region,
-    wb: &mut RocksWriteBatch,
+    wb_kv: &mut RocksWriteBatch,
 ) -> Result<()> {
     let id = region.get_id();
     let key = keys::region_state_key(id);
 
-    let region_state = db
+    let region_state = db_kv
         .c()
         .get_msg_cf::<RegionLocalState>(CF_RAFT, &key)
         .map_err(|e| box_err!(e))
@@ -1307,7 +1314,7 @@ fn set_region_tombstone(
         return Err(box_err!("The peer is still in target peers"));
     }
 
-    box_try!(write_peer_state(wb, &region, PeerState::Tombstone, None));
+    box_try!(write_peer_state(wb_kv, &region, PeerState::Tombstone, None));
     Ok(())
 }
 
@@ -1375,7 +1382,7 @@ mod tests {
         let mut apply_state = RaftApplyState::default();
         apply_state.set_applied_index(applied_index);
         apply_state.set_commit_index(commit_index);
-        kv_engine
+        raft_engine
             .put_msg_cf(CF_RAFT, &apply_state_key, &apply_state)
             .unwrap();
 
@@ -1443,6 +1450,7 @@ mod tests {
             (DBType::Kv, CF_WRITE),
             (DBType::Kv, CF_LOCK),
             (DBType::Kv, CF_RAFT),
+            (DBType::Raft, CF_RAFT),
             (DBType::Raft, CF_DEFAULT),
         ];
         for (db, cf) in valid_cases {
@@ -1452,7 +1460,6 @@ mod tests {
         let invalid_cases = vec![
             (DBType::Raft, CF_WRITE),
             (DBType::Raft, CF_LOCK),
-            (DBType::Raft, CF_RAFT),
             (DBType::Invalid, CF_DEFAULT),
             (DBType::Invalid, "BAD_CF"),
         ];
@@ -1573,11 +1580,11 @@ mod tests {
         let apply_state_key = keys::apply_state_key(region_id);
         let mut apply_state = RaftApplyState::default();
         apply_state.set_applied_index(42);
-        kv_engine
+        raft_engine
             .put_msg_cf(CF_RAFT, &apply_state_key, &apply_state)
             .unwrap();
         assert_eq!(
-            kv_engine
+            raft_engine
                 .get_msg_cf::<RaftApplyState>(CF_RAFT, &apply_state_key)
                 .unwrap()
                 .unwrap(),
@@ -1587,11 +1594,11 @@ mod tests {
         let region_state_key = keys::region_state_key(region_id);
         let mut region_state = RegionLocalState::default();
         region_state.set_state(PeerState::Tombstone);
-        kv_engine
+        raft_engine
             .put_msg_cf(CF_RAFT, &region_state_key, &region_state)
             .unwrap();
         assert_eq!(
-            kv_engine
+            raft_engine
                 .get_msg_cf::<RegionLocalState>(CF_RAFT, &region_state_key)
                 .unwrap()
                 .unwrap(),
@@ -1611,7 +1618,8 @@ mod tests {
     #[test]
     fn test_region_size() {
         let debugger = new_debugger();
-        let engine = &debugger.engines.kv;
+        let engine_kv = &debugger.engines.kv;
+        let engine_raft = &debugger.engines.raft;
 
         let region_id = 1;
         let region_state_key = keys::region_state_key(region_id);
@@ -1621,14 +1629,14 @@ mod tests {
         region.set_end_key(b"zz".to_vec());
         let mut state = RegionLocalState::default();
         state.set_region(region);
-        engine
+        engine_raft
             .put_msg_cf(CF_RAFT, &region_state_key, &state)
             .unwrap();
 
         let cfs = vec![CF_DEFAULT, CF_LOCK, CF_RAFT, CF_WRITE];
         let (k, v) = (keys::data_key(b"k"), b"v");
         for cf in &cfs {
-            engine.put_cf(cf, k.as_slice(), v).unwrap();
+            engine_kv.put_cf(cf, k.as_slice(), v).unwrap();
         }
 
         let sizes = debugger.region_size(region_id, cfs.clone()).unwrap();
@@ -1651,7 +1659,7 @@ mod tests {
     fn test_tombstone_regions() {
         let debugger = new_debugger();
         debugger.set_store_id(11);
-        let engine = &debugger.engines.kv;
+        let engine = &debugger.engines.raft;
 
         // region 1 with peers at stores 11, 12, 13.
         let region_1 = init_region_state(engine.as_inner(), 1, &[11, 12, 13], 0);
@@ -1705,7 +1713,7 @@ mod tests {
     fn test_tombstone_regions_by_id() {
         let debugger = new_debugger();
         debugger.set_store_id(11);
-        let engine = &debugger.engines.kv;
+        let engine = &debugger.engines.raft;
 
         // tombstone region 1 which currently not exists.
         let errors = debugger.set_region_tombstone_by_id(vec![1]).unwrap();
@@ -1731,7 +1739,7 @@ mod tests {
     fn test_remove_failed_stores() {
         let debugger = new_debugger();
         debugger.set_store_id(100);
-        let engine = &debugger.engines.kv;
+        let engine = &debugger.engines.raft;
 
         let get_region_stores = |engine: &Arc<DB>, region_id: u64| {
             get_region_state(engine, region_id)
diff --git a/src/server/node.rs b/src/server/node.rs
index ceb68ffea..c7dfb287b 100644
--- a/src/server/node.rs
+++ b/src/server/node.rs
@@ -229,7 +229,7 @@ where
         self.system.router()
     }
     /// Gets a transmission end of a channel which is used send messages to apply worker.
-    pub fn get_apply_router(&self) -> ApplyRouter<EK> {
+    pub fn get_apply_router(&self) -> ApplyRouter<EK, ER> {
         self.system.apply_router()
     }
 
diff --git a/tests/integrations/config/dynamic/raftstore.rs b/tests/integrations/config/dynamic/raftstore.rs
index 0bfae8716..b6567a810 100644
--- a/tests/integrations/config/dynamic/raftstore.rs
+++ b/tests/integrations/config/dynamic/raftstore.rs
@@ -63,7 +63,7 @@ fn start_raftstore(
 ) -> (
     ConfigController,
     RaftRouter<RocksEngine, RocksEngine>,
-    ApplyRouter<RocksEngine>,
+    ApplyRouter<RocksEngine, RocksEngine>,
     RaftBatchSystem<RocksEngine, RocksEngine>,
 ) {
     let (raft_router, mut system) = create_raft_batch_system(&cfg.raft_store);
diff --git a/tests/integrations/config/mod.rs b/tests/integrations/config/mod.rs
index 78f6b932f..7ab3a188c 100644
--- a/tests/integrations/config/mod.rs
+++ b/tests/integrations/config/mod.rs
@@ -248,6 +248,8 @@ fn test_serde_custom_tikv_config() {
         purge_obsolete_files_period: ReadableDuration::secs(1),
     };
     value.rocksdb = DbConfig {
+        manual_wal_flush: false,
+        fail_on_write: false,
         wal_recovery_mode: DBRecoveryMode::AbsoluteConsistency,
         wal_dir: "/var".to_owned(),
         wal_ttl_seconds: 1,
@@ -605,6 +607,71 @@ fn test_serde_custom_tikv_config() {
             bottommost_zstd_compression_dict_size: 0,
             bottommost_zstd_compression_sample_size: 0,
         },
+        raftcf: RaftCfConfig {
+            block_size: ReadableSize::kb(12),
+            block_cache_size: ReadableSize::gb(12),
+            disable_block_cache: false,
+            cache_index_and_filter_blocks: false,
+            pin_l0_filter_and_index_blocks: false,
+            use_bloom_filter: false,
+            optimize_filters_for_hits: false,
+            whole_key_filtering: true,
+            bloom_filter_bits_per_key: 123,
+            block_based_bloom_filter: true,
+            read_amp_bytes_per_bit: 0,
+            compression_per_level: [
+                DBCompressionType::No,
+                DBCompressionType::No,
+                DBCompressionType::Zstd,
+                DBCompressionType::Zstd,
+                DBCompressionType::No,
+                DBCompressionType::Zstd,
+                DBCompressionType::Lz4,
+            ],
+            write_buffer_size: ReadableSize::mb(1),
+            max_write_buffer_number: 12,
+            min_write_buffer_number_to_merge: 12,
+            max_bytes_for_level_base: ReadableSize::kb(12),
+            target_file_size_base: ReadableSize::kb(123),
+            level0_file_num_compaction_trigger: 123,
+            level0_slowdown_writes_trigger: 123,
+            level0_stop_writes_trigger: 123,
+            max_compaction_bytes: ReadableSize::gb(1),
+            compaction_pri: CompactionPriority::MinOverlappingRatio,
+            dynamic_level_bytes: true,
+            num_levels: 4,
+            max_bytes_for_level_multiplier: 8,
+            compaction_style: DBCompactionStyle::Universal,
+            disable_auto_compactions: true,
+            disable_write_stall: true,
+            soft_pending_compaction_bytes_limit: ReadableSize::gb(12),
+            hard_pending_compaction_bytes_limit: ReadableSize::gb(12),
+            force_consistency_checks: true,
+            titan: TitanCfConfig {
+                min_blob_size: ReadableSize(1024), // default value
+                blob_file_compression: CompressionType::Lz4,
+                blob_cache_size: ReadableSize::mb(0),
+                min_gc_batch_size: ReadableSize::mb(16),
+                max_gc_batch_size: ReadableSize::mb(64),
+                discardable_ratio: 0.5,
+                sample_ratio: 0.1,
+                merge_small_file_threshold: ReadableSize::mb(8),
+                blob_run_mode: BlobRunMode::ReadOnly, // default value
+                level_merge: false,
+                range_merge: true,
+                max_sorted_runs: 20,
+                gc_merge_rewrite: false,
+            },
+            prop_size_index_distance: 4000000,
+            prop_keys_index_distance: 40000,
+            enable_doubly_skiplist: true,
+            enable_compaction_guard: true,
+            compaction_guard_min_output_file_size: ReadableSize::mb(12),
+            compaction_guard_max_output_file_size: ReadableSize::mb(34),
+            bottommost_level_compression: DBCompressionType::Disable,
+            bottommost_zstd_compression_dict_size: 0,
+            bottommost_zstd_compression_sample_size: 0,
+        },
         titan: titan_db_config,
     };
     value.raft_engine.enable = true;
diff --git a/tests/integrations/raftstore/test_bootstrap.rs b/tests/integrations/raftstore/test_bootstrap.rs
index ea196245f..e92d3e5d6 100644
--- a/tests/integrations/raftstore/test_bootstrap.rs
+++ b/tests/integrations/raftstore/test_bootstrap.rs
@@ -39,7 +39,7 @@ fn test_bootstrap_idempotent<T: Simulator>(cluster: &mut Cluster<T>) {
 fn test_node_bootstrap_with_prepared_data() {
     // create a node
     let pd_client = Arc::new(TestPdClient::new(0, false));
-    let cfg = new_tikv_config(0);
+    let cfg = new_tikv_config(0, false);
 
     let (_, system) = fsm::create_raft_batch_system(&cfg.raft_store);
     let simulate_trans = SimulateTransport::new(ChannelTransport::new());
diff --git a/tests/integrations/raftstore/test_compact_log.rs b/tests/integrations/raftstore/test_compact_log.rs
index cd93f0b25..5aab114bb 100644
--- a/tests/integrations/raftstore/test_compact_log.rs
+++ b/tests/integrations/raftstore/test_compact_log.rs
@@ -14,7 +14,7 @@ fn get_raft_msg_or_default<M: protobuf::Message + Default>(
     key: &[u8],
 ) -> M {
     engines
-        .kv
+        .raft
         .get_msg_cf(CF_RAFT, key)
         .unwrap()
         .unwrap_or_default()
diff --git a/tests/integrations/raftstore/test_multi.rs b/tests/integrations/raftstore/test_multi.rs
index 0127726a9..d7c79f178 100644
--- a/tests/integrations/raftstore/test_multi.rs
+++ b/tests/integrations/raftstore/test_multi.rs
@@ -10,8 +10,8 @@ use rand::Rng;
 use kvproto::raft_cmdpb::RaftCmdResponse;
 use raft::eraftpb::MessageType;
 
-use engine_rocks::Compat;
-use engine_traits::Peekable;
+use engine_rocks::{Compat, RocksSnapshot};
+use engine_traits::{Peekable, Mutable, KvEngine, CF_RAFT, WriteBatch, WriteBatchExt};
 use raftstore::router::RaftStoreRouter;
 use raftstore::store::*;
 use raftstore::Result;
@@ -822,3 +822,201 @@ fn test_node_catch_up_logs() {
     cluster.run_node(3).unwrap();
     must_get_equal(&cluster.get_engine(3), b"0009", b"0009");
 }
+
+#[test]
+fn test_rocksdb_restart_leader() {
+    let mut cluster = new_node_cluster(0, 3);
+    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(500);
+    cluster.cfg.raft_store.raft_max_size_per_msg = ReadableSize(5);
+    cluster.cfg.raft_store.raft_election_timeout_ticks = 50;
+    cluster.cfg.raft_store.max_leader_missing_duration = ReadableDuration::hours(1);
+    cluster.cfg.raft_store.peer_stale_state_check_interval = ReadableDuration::minutes(30);
+    cluster.cfg.raft_store.abnormal_leader_missing_duration = ReadableDuration::hours(1);
+    // disable compact log to make test more stable.
+    cluster.cfg.raft_store.raft_log_gc_threshold = 3000;
+    cluster.pd_client.disable_default_operator();
+    // We use three peers([1, 2, 3]) for this test.
+    let r1 = cluster.run_conf_change();
+    cluster.pd_client.must_add_peer(r1, new_peer(2, 2));
+    cluster.pd_client.must_add_peer(r1, new_peer(3, 3));
+
+    cluster.must_put(b"k1", b"v1");
+    for i in 0..10 {
+        let v = format!("{:04}", i);
+        cluster.async_put(v.as_bytes(), v.as_bytes()).unwrap();
+    }
+    must_get_equal(&cluster.get_engine(1), b"0009", b"0009");
+    cluster.shutdown();
+    cluster.start().unwrap();
+        
+//    cluster.stop_node(1);
+//    cluster.run_node(1).unwrap();
+    must_get_equal(&cluster.get_engine(1), b"0009", b"0009");
+}
+
+#[test]
+fn test_rocksdb_durability_shutdown() {
+    let mut cluster = new_node_cluster(0, 1);
+    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(500);
+    cluster.cfg.raft_store.raft_max_size_per_msg = ReadableSize(5);
+    cluster.cfg.raft_store.raft_election_timeout_ticks = 50;
+    cluster.cfg.raft_store.max_leader_missing_duration = ReadableDuration::hours(1);
+    cluster.cfg.raft_store.peer_stale_state_check_interval = ReadableDuration::minutes(30);
+    cluster.cfg.raft_store.abnormal_leader_missing_duration = ReadableDuration::hours(1);
+    // disable compact log to make test more stable.
+    cluster.cfg.raft_store.raft_log_gc_threshold = 3000;
+    cluster.pd_client.disable_default_operator();
+
+    cluster.run_conf_change();
+
+    cluster.must_put(b"k1", b"v1");
+    for i in 0..10 {
+        let v = format!("{:04}", i);
+        cluster.async_put(v.as_bytes(), v.as_bytes()).unwrap();
+    }
+    must_get_equal(&cluster.get_engine(1), b"0009", b"0009");
+    cluster.shutdown();
+    cluster.start().unwrap();
+        
+//    cluster.stop_node(1);
+//    cluster.run_node(1).unwrap();
+    must_get_equal(&cluster.get_engine(1), b"0009", b"0009");
+
+}
+
+#[test]
+fn test_cluster_restart_normal() {
+    let mut cluster = new_node_cluster(0, 1);
+    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(500);
+    cluster.cfg.raft_store.raft_max_size_per_msg = ReadableSize(5);
+    cluster.cfg.raft_store.raft_election_timeout_ticks = 50;
+    cluster.cfg.raft_store.max_leader_missing_duration = ReadableDuration::hours(1);
+    cluster.cfg.raft_store.peer_stale_state_check_interval = ReadableDuration::minutes(30);
+    cluster.cfg.raft_store.abnormal_leader_missing_duration = ReadableDuration::hours(1);
+    // disable compact log to make test more stable.
+    cluster.cfg.raft_store.raft_log_gc_threshold = 3000;
+    cluster.pd_client.disable_default_operator();
+
+    cluster.run_conf_change();
+
+    cluster.must_put(b"k1", b"v1");
+    for i in 0..10 {
+        let v = format!("{:04}", i);
+        cluster.async_put(v.as_bytes(), v.as_bytes()).unwrap();
+    }
+    must_get_equal(&cluster.get_engine(1), b"0009", b"0009");
+
+    sleep_ms(500);    
+    cluster.stop_node(1);
+    cluster.reboot_engine(1);
+    cluster.run_node(1).unwrap();
+        
+//    cluster.stop_node(1);
+//    cluster.run_node(1).unwrap();
+    must_get_equal(&cluster.get_engine(1), b"0009", b"0009");
+}
+
+#[test]
+fn test_cluster_wal_sync_fault() {
+    let mut cluster = new_fault_node_cluster(0, 1);
+    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(500);
+    cluster.cfg.raft_store.raft_max_size_per_msg = ReadableSize(5);
+    cluster.cfg.raft_store.raft_election_timeout_ticks = 50;
+    cluster.cfg.raft_store.max_leader_missing_duration = ReadableDuration::hours(1);
+    cluster.cfg.raft_store.peer_stale_state_check_interval = ReadableDuration::minutes(30);
+    cluster.cfg.raft_store.abnormal_leader_missing_duration = ReadableDuration::hours(1);
+    // disable compact log to make test more stable.
+    cluster.cfg.raft_store.raft_log_gc_threshold = 3000;
+    cluster.pd_client.disable_default_operator();
+
+    cluster.run_conf_change();
+
+    for i in 0..5 {
+        let v = format!("{:04}", i);
+        cluster.async_put(v.as_bytes(), v.as_bytes()).unwrap();
+    }
+    must_get_equal(&cluster.get_engine(1), b"0004", b"0004");
+
+    sleep_ms(500);    
+    cluster.stop_node(1);
+    cluster.reboot_engine(1);
+    cluster.run_node(1).unwrap();
+        
+//    cluster.stop_node(1);
+//    cluster.run_node(1).unwrap();
+    must_get_none(&cluster.get_engine(1), b"0009");
+}
+
+
+#[test]
+fn test_cluster_restart_failwrite() {
+    let mut cluster = new_fault_node_cluster(0, 1);
+    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(500);
+    cluster.cfg.raft_store.raft_max_size_per_msg = ReadableSize(5);
+    cluster.cfg.raft_store.raft_election_timeout_ticks = 50;
+    cluster.cfg.raft_store.max_leader_missing_duration = ReadableDuration::hours(1);
+    cluster.cfg.raft_store.peer_stale_state_check_interval = ReadableDuration::minutes(30);
+    cluster.cfg.raft_store.abnormal_leader_missing_duration = ReadableDuration::hours(1);
+    // disable compact log to make test more stable.
+    cluster.cfg.raft_store.raft_log_gc_threshold = 3000;
+    cluster.pd_client.disable_default_operator();
+
+    cluster.run_conf_change();
+
+    cluster.must_put(b"k1", b"v1");
+    for i in 0..10 {
+        let v = format!("{:04}", i);
+        cluster.async_put(v.as_bytes(), v.as_bytes()).unwrap();
+    }
+    must_get_equal(&cluster.get_engine(1), b"0009", b"0009");
+
+    sleep_ms(500);    
+    cluster.stop_node(1);
+    cluster.reboot_engine(1);
+    cluster.run_node(1).unwrap();
+        
+//    cluster.stop_node(1);
+//    cluster.run_node(1).unwrap();
+    must_get_equal(&cluster.get_engine(1), b"0009", b"0009");
+}
+
+#[test]
+fn test_cluster_restart_reapply_raft_log() {
+    // setup and run 1-node cluster
+    let mut cluster = new_node_cluster(0, 1);
+    cluster.pd_client.disable_default_operator();
+    // So compact log will not be triggered automatically.
+    configure_for_request_snapshot(&mut cluster);
+    cluster.run();
+
+    // put k1
+    cluster.must_put(b"k1", b"v1");
+    must_get_equal(&cluster.get_engine(1), b"k1", b"v1");
+    let apply_state0 = cluster.apply_state(1, 1);
+    print!{"ApplyState0: commit: {}, apply: {}\n", apply_state0.get_commit_index(), apply_state0.get_applied_index()};
+
+    // put k2
+    cluster.must_put(b"k2", b"v2");
+    must_get_equal(&cluster.get_engine(1), b"k2", b"v2");
+    // stop node
+    cluster.stop_node(1);
+
+    // now let's undo that last put operation... 
+    // revert ApplyState
+    let mut wb = cluster.get_all_engines(1).kv.write_batch();
+    wb.put_msg_cf(CF_RAFT, &keys::apply_state_key(1), &apply_state0).unwrap();
+    let mut write_opts = engine_traits::WriteOptions::new();
+    write_opts.set_sync(false);
+    write_opts.set_disable_wal(true);
+    wb.write_opt(&write_opts).unwrap();
+
+    // delete k2
+    must_delete(&cluster.get_engine(1), b"k2");
+    must_get_none(&cluster.get_engine(1), b"k2");
+    cluster.run_node(1).unwrap();
+    
+    sleep_ms(1000); // raftstore will just do its thing on reboot.
+    // k1 and k2 should both be present after raftstore reboot
+    must_get_equal(&cluster.get_engine(1), b"k1", b"v1");
+    must_get_equal(&cluster.get_engine(1), b"k2", b"v2");
+}
diff --git a/tests/integrations/server/kv_service.rs b/tests/integrations/server/kv_service.rs
index eadab5df8..50e0fddd1 100644
--- a/tests/integrations/server/kv_service.rs
+++ b/tests/integrations/server/kv_service.rs
@@ -762,12 +762,12 @@ fn test_debug_region_info() {
     let apply_state_key = keys::apply_state_key(region_id);
     let mut apply_state = raft_serverpb::RaftApplyState::default();
     apply_state.set_applied_index(42);
-    kv_engine
+    raft_engine
         .c()
         .put_msg_cf(CF_RAFT, &apply_state_key, &apply_state)
         .unwrap();
     assert_eq!(
-        kv_engine
+        raft_engine
             .c()
             .get_msg_cf::<raft_serverpb::RaftApplyState>(CF_RAFT, &apply_state_key)
             .unwrap()
@@ -778,12 +778,12 @@ fn test_debug_region_info() {
     let region_state_key = keys::region_state_key(region_id);
     let mut region_state = raft_serverpb::RegionLocalState::default();
     region_state.set_state(raft_serverpb::PeerState::Tombstone);
-    kv_engine
+    raft_engine
         .c()
         .put_msg_cf(CF_RAFT, &region_state_key, &region_state)
         .unwrap();
     assert_eq!(
-        kv_engine
+        raft_engine
             .c()
             .get_msg_cf::<raft_serverpb::RegionLocalState>(CF_RAFT, &region_state_key)
             .unwrap()
